{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Text_Gen_RNN/blob/model/CharRNN%2BWordRNN_Infinite_Joke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W8R8WgZceEk"
      },
      "source": [
        "# Character-Level LSTM\n",
        "Обучим модель на тексте книги \"Бесконечная шутка\", после чего попробуем генерировать новый текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqUOE2flceEl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4gZYNppMOo0"
      },
      "outputs": [],
      "source": [
        "# фиксируем воспроизводимость\n",
        "def seed_everything(seed=42):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed=7575)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wHfCDyzceEl"
      },
      "source": [
        "## Загрузим данные\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b34kfqIOceEl"
      },
      "outputs": [],
      "source": [
        "# open text file and read in data as `text`\n",
        "with open(\"/content/Uolles_D._Velikieromanyi._Beskonechnaya_Shutka.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      },
      "source": [
        "Посмотрим первые 100 символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7VctmLQfceEl",
        "outputId": "840e9158-2767-48a4-a24e-5e13bce85415"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Бесконечная шутка\\nДэвид Фостер Уоллес\\n\\n\\nВеликие романы\\nВ недалеком будущем пациенты реабилитационной'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iC21bopceEl"
      },
      "source": [
        "### Токенизация\n",
        "\n",
        "В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYVlmnxLceEl"
      },
      "outputs": [],
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJIzwzSwceEl"
      },
      "source": [
        "Посмотрим как символы закодировались целыми числами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WK1MYr_9ceEl",
        "outputId": "e4308654-9d42-4a2c-f0bd-faf017de0d23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([143,  90, 112, 137, 146,  29,  90,   0,  29, 115, 103,  72, 154,\n",
              "        49, 129, 137, 115, 127, 140,  78,  36,  23, 152,  72, 107, 146,\n",
              "       112, 129,  90, 156,  72,  44, 146,   5,   5,  90, 112, 127, 127,\n",
              "       127,  76,  90,   5,  23, 137,  23,  90,  72, 156, 146,  25, 115,\n",
              "        29,  99, 127,  76,  72,  29,  90, 152, 115,   5,  90, 137, 146,\n",
              "        25,  72,  17,  49, 152,  49,  27,  90,  25,  72,  94, 115, 145,\n",
              "        23,  90,  29, 129,  99,  72, 156,  90, 115,  17,  23,   5,  23,\n",
              "       129, 115, 145,  23, 146,  29,  29, 146, 170])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "encoded[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azltQy-gceEl"
      },
      "source": [
        "## Предобработка данных\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnahALhiceEl"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLBOH8ZQUl7B"
      },
      "outputs": [],
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Generates batches from encoded sequence.\n",
        "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
        "    :param batch_size: number of sequences per batch\n",
        "    :param seq_length: number of encoded chars in a sequence\n",
        "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
        "    \"\"\"\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlTnDntHceEl",
        "outputId": "82a7b389-26e1-4525-bd5d-c0a5c0a9ccba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPq1EA38rBqn"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IrBRlEPceEl"
      },
      "source": [
        "## Обучим модель\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv8VkRI0ceEl"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    \"\"\"Training a network\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    net: CharRNN network\n",
        "    data: text data to train the network\n",
        "    epochs: Number of epochs to train\n",
        "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "    seq_length: Number of character steps per mini-batch\n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt0q4KGEceEm"
      },
      "source": [
        "## Определим модель\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykMcIloEr3G7",
        "outputId": "0ab1240a-84c9-4a89-fed6-819a30273d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(172, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=172, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHy6mECuceEm"
      },
      "source": [
        "### Установим гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hTkNrWEsjgI",
        "outputId": "001be050-f4bb-4742-ba81-4a8034b6d566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 3.4749... Val Loss: 3.5546\n",
            "Epoch: 1/50... Step: 20... Loss: 3.4495... Val Loss: 3.4744\n",
            "Epoch: 1/50... Step: 30... Loss: 3.3736... Val Loss: 3.4520\n",
            "Epoch: 1/50... Step: 40... Loss: 3.3488... Val Loss: 3.4415\n",
            "Epoch: 1/50... Step: 50... Loss: 3.3693... Val Loss: 3.4433\n",
            "Epoch: 1/50... Step: 60... Loss: 3.3651... Val Loss: 3.4376\n",
            "Epoch: 1/50... Step: 70... Loss: 3.3331... Val Loss: 3.4343\n",
            "Epoch: 1/50... Step: 80... Loss: 3.3756... Val Loss: 3.4207\n",
            "Epoch: 1/50... Step: 90... Loss: 3.3133... Val Loss: 3.4223\n",
            "Epoch: 1/50... Step: 100... Loss: 3.3390... Val Loss: 3.4129\n",
            "Epoch: 1/50... Step: 110... Loss: 3.3061... Val Loss: 3.4019\n",
            "Epoch: 2/50... Step: 120... Loss: 3.2897... Val Loss: 3.3898\n",
            "Epoch: 2/50... Step: 130... Loss: 3.2949... Val Loss: 3.3456\n",
            "Epoch: 2/50... Step: 140... Loss: 3.2434... Val Loss: 3.2764\n",
            "Epoch: 2/50... Step: 150... Loss: 3.1525... Val Loss: 3.1925\n",
            "Epoch: 2/50... Step: 160... Loss: 3.0471... Val Loss: 3.0922\n",
            "Epoch: 2/50... Step: 170... Loss: 2.9919... Val Loss: 3.0354\n",
            "Epoch: 2/50... Step: 180... Loss: 2.9827... Val Loss: 2.9728\n",
            "Epoch: 2/50... Step: 190... Loss: 2.9248... Val Loss: 2.9277\n",
            "Epoch: 2/50... Step: 200... Loss: 2.8611... Val Loss: 2.8848\n",
            "Epoch: 2/50... Step: 210... Loss: 2.8443... Val Loss: 2.8608\n",
            "Epoch: 2/50... Step: 220... Loss: 2.7974... Val Loss: 2.8247\n",
            "Epoch: 3/50... Step: 230... Loss: 2.7743... Val Loss: 2.7936\n",
            "Epoch: 3/50... Step: 240... Loss: 2.7483... Val Loss: 2.7741\n",
            "Epoch: 3/50... Step: 250... Loss: 2.7643... Val Loss: 2.7453\n",
            "Epoch: 3/50... Step: 260... Loss: 2.7081... Val Loss: 2.7196\n",
            "Epoch: 3/50... Step: 270... Loss: 2.6733... Val Loss: 2.6977\n",
            "Epoch: 3/50... Step: 280... Loss: 2.6736... Val Loss: 2.6775\n",
            "Epoch: 3/50... Step: 290... Loss: 2.6553... Val Loss: 2.6594\n",
            "Epoch: 3/50... Step: 300... Loss: 2.6161... Val Loss: 2.6426\n",
            "Epoch: 3/50... Step: 310... Loss: 2.6055... Val Loss: 2.6263\n",
            "Epoch: 3/50... Step: 320... Loss: 2.6257... Val Loss: 2.6142\n",
            "Epoch: 3/50... Step: 330... Loss: 2.5761... Val Loss: 2.6031\n",
            "Epoch: 4/50... Step: 340... Loss: 2.5806... Val Loss: 2.5888\n",
            "Epoch: 4/50... Step: 350... Loss: 2.5841... Val Loss: 2.5711\n",
            "Epoch: 4/50... Step: 360... Loss: 2.5521... Val Loss: 2.5597\n",
            "Epoch: 4/50... Step: 370... Loss: 2.5480... Val Loss: 2.5456\n",
            "Epoch: 4/50... Step: 380... Loss: 2.5331... Val Loss: 2.5294\n",
            "Epoch: 4/50... Step: 390... Loss: 2.5176... Val Loss: 2.5116\n",
            "Epoch: 4/50... Step: 400... Loss: 2.4904... Val Loss: 2.5045\n",
            "Epoch: 4/50... Step: 410... Loss: 2.4965... Val Loss: 2.4870\n",
            "Epoch: 4/50... Step: 420... Loss: 2.4794... Val Loss: 2.4832\n",
            "Epoch: 4/50... Step: 430... Loss: 2.4870... Val Loss: 2.4682\n",
            "Epoch: 4/50... Step: 440... Loss: 2.4534... Val Loss: 2.4545\n",
            "Epoch: 5/50... Step: 450... Loss: 2.4676... Val Loss: 2.4407\n",
            "Epoch: 5/50... Step: 460... Loss: 2.4344... Val Loss: 2.4327\n",
            "Epoch: 5/50... Step: 470... Loss: 2.4376... Val Loss: 2.4228\n",
            "Epoch: 5/50... Step: 480... Loss: 2.4300... Val Loss: 2.4078\n",
            "Epoch: 5/50... Step: 490... Loss: 2.4086... Val Loss: 2.3932\n",
            "Epoch: 5/50... Step: 500... Loss: 2.3906... Val Loss: 2.3902\n",
            "Epoch: 5/50... Step: 510... Loss: 2.3866... Val Loss: 2.3749\n",
            "Epoch: 5/50... Step: 520... Loss: 2.3951... Val Loss: 2.3712\n",
            "Epoch: 5/50... Step: 530... Loss: 2.3635... Val Loss: 2.3574\n",
            "Epoch: 5/50... Step: 540... Loss: 2.3637... Val Loss: 2.3478\n",
            "Epoch: 5/50... Step: 550... Loss: 2.3312... Val Loss: 2.3378\n",
            "Epoch: 5/50... Step: 560... Loss: 2.3492... Val Loss: 2.3315\n",
            "Epoch: 6/50... Step: 570... Loss: 2.3303... Val Loss: 2.3264\n",
            "Epoch: 6/50... Step: 580... Loss: 2.3500... Val Loss: 2.3160\n",
            "Epoch: 6/50... Step: 590... Loss: 2.3303... Val Loss: 2.3046\n",
            "Epoch: 6/50... Step: 600... Loss: 2.3085... Val Loss: 2.2956\n",
            "Epoch: 6/50... Step: 610... Loss: 2.3234... Val Loss: 2.2911\n",
            "Epoch: 6/50... Step: 620... Loss: 2.3151... Val Loss: 2.2845\n",
            "Epoch: 6/50... Step: 630... Loss: 2.3103... Val Loss: 2.2742\n",
            "Epoch: 6/50... Step: 640... Loss: 2.2788... Val Loss: 2.2662\n",
            "Epoch: 6/50... Step: 650... Loss: 2.2587... Val Loss: 2.2576\n",
            "Epoch: 6/50... Step: 660... Loss: 2.2801... Val Loss: 2.2537\n",
            "Epoch: 6/50... Step: 670... Loss: 2.2634... Val Loss: 2.2453\n",
            "Epoch: 7/50... Step: 680... Loss: 2.2494... Val Loss: 2.2436\n",
            "Epoch: 7/50... Step: 690... Loss: 2.2595... Val Loss: 2.2376\n",
            "Epoch: 7/50... Step: 700... Loss: 2.2618... Val Loss: 2.2267\n",
            "Epoch: 7/50... Step: 710... Loss: 2.2483... Val Loss: 2.2150\n",
            "Epoch: 7/50... Step: 720... Loss: 2.2552... Val Loss: 2.2093\n",
            "Epoch: 7/50... Step: 730... Loss: 2.2494... Val Loss: 2.2020\n",
            "Epoch: 7/50... Step: 740... Loss: 2.2524... Val Loss: 2.1979\n",
            "Epoch: 7/50... Step: 750... Loss: 2.2186... Val Loss: 2.1879\n",
            "Epoch: 7/50... Step: 760... Loss: 2.2172... Val Loss: 2.1860\n",
            "Epoch: 7/50... Step: 770... Loss: 2.2221... Val Loss: 2.1805\n",
            "Epoch: 7/50... Step: 780... Loss: 2.1950... Val Loss: 2.1746\n",
            "Epoch: 8/50... Step: 790... Loss: 2.2065... Val Loss: 2.1673\n",
            "Epoch: 8/50... Step: 800... Loss: 2.2021... Val Loss: 2.1640\n",
            "Epoch: 8/50... Step: 810... Loss: 2.2076... Val Loss: 2.1580\n",
            "Epoch: 8/50... Step: 820... Loss: 2.1878... Val Loss: 2.1515\n",
            "Epoch: 8/50... Step: 830... Loss: 2.1776... Val Loss: 2.1431\n",
            "Epoch: 8/50... Step: 840... Loss: 2.1678... Val Loss: 2.1370\n",
            "Epoch: 8/50... Step: 850... Loss: 2.1701... Val Loss: 2.1323\n",
            "Epoch: 8/50... Step: 860... Loss: 2.1435... Val Loss: 2.1250\n",
            "Epoch: 8/50... Step: 870... Loss: 2.1436... Val Loss: 2.1230\n",
            "Epoch: 8/50... Step: 880... Loss: 2.1774... Val Loss: 2.1202\n",
            "Epoch: 8/50... Step: 890... Loss: 2.1254... Val Loss: 2.1119\n",
            "Epoch: 9/50... Step: 900... Loss: 2.1611... Val Loss: 2.1096\n",
            "Epoch: 9/50... Step: 910... Loss: 2.1569... Val Loss: 2.1012\n",
            "Epoch: 9/50... Step: 920... Loss: 2.1410... Val Loss: 2.0977\n",
            "Epoch: 9/50... Step: 930... Loss: 2.1325... Val Loss: 2.0910\n",
            "Epoch: 9/50... Step: 940... Loss: 2.1486... Val Loss: 2.0849\n",
            "Epoch: 9/50... Step: 950... Loss: 2.1283... Val Loss: 2.0781\n",
            "Epoch: 9/50... Step: 960... Loss: 2.1128... Val Loss: 2.0754\n",
            "Epoch: 9/50... Step: 970... Loss: 2.1160... Val Loss: 2.0713\n",
            "Epoch: 9/50... Step: 980... Loss: 2.1075... Val Loss: 2.0673\n",
            "Epoch: 9/50... Step: 990... Loss: 2.1228... Val Loss: 2.0645\n",
            "Epoch: 9/50... Step: 1000... Loss: 2.0854... Val Loss: 2.0586\n",
            "Epoch: 10/50... Step: 1010... Loss: 2.1254... Val Loss: 2.0532\n",
            "Epoch: 10/50... Step: 1020... Loss: 2.0814... Val Loss: 2.0514\n",
            "Epoch: 10/50... Step: 1030... Loss: 2.0894... Val Loss: 2.0455\n",
            "Epoch: 10/50... Step: 1040... Loss: 2.0928... Val Loss: 2.0397\n",
            "Epoch: 10/50... Step: 1050... Loss: 2.0788... Val Loss: 2.0331\n",
            "Epoch: 10/50... Step: 1060... Loss: 2.0698... Val Loss: 2.0296\n",
            "Epoch: 10/50... Step: 1070... Loss: 2.0512... Val Loss: 2.0213\n",
            "Epoch: 10/50... Step: 1080... Loss: 2.0679... Val Loss: 2.0219\n",
            "Epoch: 10/50... Step: 1090... Loss: 2.0598... Val Loss: 2.0187\n",
            "Epoch: 10/50... Step: 1100... Loss: 2.0571... Val Loss: 2.0168\n",
            "Epoch: 10/50... Step: 1110... Loss: 2.0244... Val Loss: 2.0104\n",
            "Epoch: 10/50... Step: 1120... Loss: 2.0453... Val Loss: 2.0072\n",
            "Epoch: 11/50... Step: 1130... Loss: 2.0363... Val Loss: 2.0046\n",
            "Epoch: 11/50... Step: 1140... Loss: 2.0510... Val Loss: 2.0015\n",
            "Epoch: 11/50... Step: 1150... Loss: 2.0419... Val Loss: 1.9963\n",
            "Epoch: 11/50... Step: 1160... Loss: 2.0323... Val Loss: 1.9891\n",
            "Epoch: 11/50... Step: 1170... Loss: 2.0503... Val Loss: 1.9861\n",
            "Epoch: 11/50... Step: 1180... Loss: 2.0344... Val Loss: 1.9813\n",
            "Epoch: 11/50... Step: 1190... Loss: 2.0342... Val Loss: 1.9809\n",
            "Epoch: 11/50... Step: 1200... Loss: 2.0113... Val Loss: 1.9752\n",
            "Epoch: 11/50... Step: 1210... Loss: 1.9947... Val Loss: 1.9728\n",
            "Epoch: 11/50... Step: 1220... Loss: 2.0132... Val Loss: 1.9700\n",
            "Epoch: 11/50... Step: 1230... Loss: 1.9997... Val Loss: 1.9658\n",
            "Epoch: 12/50... Step: 1240... Loss: 1.9768... Val Loss: 1.9595\n",
            "Epoch: 12/50... Step: 1250... Loss: 2.0061... Val Loss: 1.9572\n",
            "Epoch: 12/50... Step: 1260... Loss: 2.0077... Val Loss: 1.9530\n",
            "Epoch: 12/50... Step: 1270... Loss: 2.0120... Val Loss: 1.9486\n",
            "Epoch: 12/50... Step: 1280... Loss: 2.0095... Val Loss: 1.9444\n",
            "Epoch: 12/50... Step: 1290... Loss: 1.9956... Val Loss: 1.9390\n",
            "Epoch: 12/50... Step: 1300... Loss: 1.9999... Val Loss: 1.9372\n",
            "Epoch: 12/50... Step: 1310... Loss: 1.9858... Val Loss: 1.9333\n",
            "Epoch: 12/50... Step: 1320... Loss: 1.9729... Val Loss: 1.9339\n",
            "Epoch: 12/50... Step: 1330... Loss: 1.9847... Val Loss: 1.9298\n",
            "Epoch: 12/50... Step: 1340... Loss: 1.9610... Val Loss: 1.9253\n",
            "Epoch: 13/50... Step: 1350... Loss: 1.9767... Val Loss: 1.9250\n",
            "Epoch: 13/50... Step: 1360... Loss: 1.9748... Val Loss: 1.9202\n",
            "Epoch: 13/50... Step: 1370... Loss: 1.9859... Val Loss: 1.9169\n",
            "Epoch: 13/50... Step: 1380... Loss: 1.9594... Val Loss: 1.9114\n",
            "Epoch: 13/50... Step: 1390... Loss: 1.9560... Val Loss: 1.9092\n",
            "Epoch: 13/50... Step: 1400... Loss: 1.9605... Val Loss: 1.9046\n",
            "Epoch: 13/50... Step: 1410... Loss: 1.9397... Val Loss: 1.9018\n",
            "Epoch: 13/50... Step: 1420... Loss: 1.9266... Val Loss: 1.8999\n",
            "Epoch: 13/50... Step: 1430... Loss: 1.9315... Val Loss: 1.8992\n",
            "Epoch: 13/50... Step: 1440... Loss: 1.9685... Val Loss: 1.8963\n",
            "Epoch: 13/50... Step: 1450... Loss: 1.9141... Val Loss: 1.8940\n",
            "Epoch: 14/50... Step: 1460... Loss: 1.9597... Val Loss: 1.8904\n",
            "Epoch: 14/50... Step: 1470... Loss: 1.9597... Val Loss: 1.8875\n",
            "Epoch: 14/50... Step: 1480... Loss: 1.9367... Val Loss: 1.8829\n",
            "Epoch: 14/50... Step: 1490... Loss: 1.9353... Val Loss: 1.8800\n",
            "Epoch: 14/50... Step: 1500... Loss: 1.9464... Val Loss: 1.8775\n",
            "Epoch: 14/50... Step: 1510... Loss: 1.9297... Val Loss: 1.8725\n",
            "Epoch: 14/50... Step: 1520... Loss: 1.9098... Val Loss: 1.8692\n",
            "Epoch: 14/50... Step: 1530... Loss: 1.9172... Val Loss: 1.8662\n",
            "Epoch: 14/50... Step: 1540... Loss: 1.9070... Val Loss: 1.8661\n",
            "Epoch: 14/50... Step: 1550... Loss: 1.9366... Val Loss: 1.8659\n",
            "Epoch: 14/50... Step: 1560... Loss: 1.8994... Val Loss: 1.8608\n",
            "Epoch: 15/50... Step: 1570... Loss: 1.9367... Val Loss: 1.8574\n",
            "Epoch: 15/50... Step: 1580... Loss: 1.8848... Val Loss: 1.8590\n",
            "Epoch: 15/50... Step: 1590... Loss: 1.9140... Val Loss: 1.8554\n",
            "Epoch: 15/50... Step: 1600... Loss: 1.9108... Val Loss: 1.8516\n",
            "Epoch: 15/50... Step: 1610... Loss: 1.8943... Val Loss: 1.8471\n",
            "Epoch: 15/50... Step: 1620... Loss: 1.8841... Val Loss: 1.8445\n",
            "Epoch: 15/50... Step: 1630... Loss: 1.8682... Val Loss: 1.8423\n",
            "Epoch: 15/50... Step: 1640... Loss: 1.8879... Val Loss: 1.8399\n",
            "Epoch: 15/50... Step: 1650... Loss: 1.8833... Val Loss: 1.8391\n",
            "Epoch: 15/50... Step: 1660... Loss: 1.8770... Val Loss: 1.8380\n",
            "Epoch: 15/50... Step: 1670... Loss: 1.8601... Val Loss: 1.8342\n",
            "Epoch: 15/50... Step: 1680... Loss: 1.8646... Val Loss: 1.8341\n",
            "Epoch: 16/50... Step: 1690... Loss: 1.8653... Val Loss: 1.8301\n",
            "Epoch: 16/50... Step: 1700... Loss: 1.8794... Val Loss: 1.8294\n",
            "Epoch: 16/50... Step: 1710... Loss: 1.8738... Val Loss: 1.8270\n",
            "Epoch: 16/50... Step: 1720... Loss: 1.8659... Val Loss: 1.8224\n",
            "Epoch: 16/50... Step: 1730... Loss: 1.8815... Val Loss: 1.8190\n",
            "Epoch: 16/50... Step: 1740... Loss: 1.8714... Val Loss: 1.8157\n",
            "Epoch: 16/50... Step: 1750... Loss: 1.8746... Val Loss: 1.8169\n",
            "Epoch: 16/50... Step: 1760... Loss: 1.8532... Val Loss: 1.8116\n",
            "Epoch: 16/50... Step: 1770... Loss: 1.8315... Val Loss: 1.8131\n",
            "Epoch: 16/50... Step: 1780... Loss: 1.8571... Val Loss: 1.8130\n",
            "Epoch: 16/50... Step: 1790... Loss: 1.8435... Val Loss: 1.8081\n",
            "Epoch: 17/50... Step: 1800... Loss: 1.8253... Val Loss: 1.8055\n",
            "Epoch: 17/50... Step: 1810... Loss: 1.8424... Val Loss: 1.8064\n",
            "Epoch: 17/50... Step: 1820... Loss: 1.8577... Val Loss: 1.8023\n",
            "Epoch: 17/50... Step: 1830... Loss: 1.8597... Val Loss: 1.7970\n",
            "Epoch: 17/50... Step: 1840... Loss: 1.8481... Val Loss: 1.7953\n",
            "Epoch: 17/50... Step: 1850... Loss: 1.8462... Val Loss: 1.7936\n",
            "Epoch: 17/50... Step: 1860... Loss: 1.8517... Val Loss: 1.7931\n",
            "Epoch: 17/50... Step: 1870... Loss: 1.8337... Val Loss: 1.7907\n",
            "Epoch: 17/50... Step: 1880... Loss: 1.8182... Val Loss: 1.7919\n",
            "Epoch: 17/50... Step: 1890... Loss: 1.8376... Val Loss: 1.7880\n",
            "Epoch: 17/50... Step: 1900... Loss: 1.8282... Val Loss: 1.7884\n",
            "Epoch: 18/50... Step: 1910... Loss: 1.8325... Val Loss: 1.7850\n",
            "Epoch: 18/50... Step: 1920... Loss: 1.8367... Val Loss: 1.7843\n",
            "Epoch: 18/50... Step: 1930... Loss: 1.8453... Val Loss: 1.7800\n",
            "Epoch: 18/50... Step: 1940... Loss: 1.8118... Val Loss: 1.7767\n",
            "Epoch: 18/50... Step: 1950... Loss: 1.8078... Val Loss: 1.7757\n",
            "Epoch: 18/50... Step: 1960... Loss: 1.8249... Val Loss: 1.7720\n",
            "Epoch: 18/50... Step: 1970... Loss: 1.8032... Val Loss: 1.7707\n",
            "Epoch: 18/50... Step: 1980... Loss: 1.7969... Val Loss: 1.7724\n",
            "Epoch: 18/50... Step: 1990... Loss: 1.8034... Val Loss: 1.7708\n",
            "Epoch: 18/50... Step: 2000... Loss: 1.8397... Val Loss: 1.7699\n",
            "Epoch: 18/50... Step: 2010... Loss: 1.7761... Val Loss: 1.7688\n",
            "Epoch: 19/50... Step: 2020... Loss: 1.8214... Val Loss: 1.7677\n",
            "Epoch: 19/50... Step: 2030... Loss: 1.8331... Val Loss: 1.7626\n",
            "Epoch: 19/50... Step: 2040... Loss: 1.8044... Val Loss: 1.7613\n",
            "Epoch: 19/50... Step: 2050... Loss: 1.8017... Val Loss: 1.7574\n",
            "Epoch: 19/50... Step: 2060... Loss: 1.8119... Val Loss: 1.7570\n",
            "Epoch: 19/50... Step: 2070... Loss: 1.8021... Val Loss: 1.7556\n",
            "Epoch: 19/50... Step: 2080... Loss: 1.7897... Val Loss: 1.7521\n",
            "Epoch: 19/50... Step: 2090... Loss: 1.7894... Val Loss: 1.7530\n",
            "Epoch: 19/50... Step: 2100... Loss: 1.7905... Val Loss: 1.7510\n",
            "Epoch: 19/50... Step: 2110... Loss: 1.8125... Val Loss: 1.7520\n",
            "Epoch: 19/50... Step: 2120... Loss: 1.7768... Val Loss: 1.7489\n",
            "Epoch: 20/50... Step: 2130... Loss: 1.8186... Val Loss: 1.7474\n",
            "Epoch: 20/50... Step: 2140... Loss: 1.7679... Val Loss: 1.7448\n",
            "Epoch: 20/50... Step: 2150... Loss: 1.7910... Val Loss: 1.7440\n",
            "Epoch: 20/50... Step: 2160... Loss: 1.7882... Val Loss: 1.7417\n",
            "Epoch: 20/50... Step: 2170... Loss: 1.7752... Val Loss: 1.7393\n",
            "Epoch: 20/50... Step: 2180... Loss: 1.7714... Val Loss: 1.7379\n",
            "Epoch: 20/50... Step: 2190... Loss: 1.7601... Val Loss: 1.7354\n",
            "Epoch: 20/50... Step: 2200... Loss: 1.7736... Val Loss: 1.7376\n",
            "Epoch: 20/50... Step: 2210... Loss: 1.7688... Val Loss: 1.7348\n",
            "Epoch: 20/50... Step: 2220... Loss: 1.7564... Val Loss: 1.7355\n",
            "Epoch: 20/50... Step: 2230... Loss: 1.7509... Val Loss: 1.7351\n",
            "Epoch: 20/50... Step: 2240... Loss: 1.7572... Val Loss: 1.7304\n",
            "Epoch: 21/50... Step: 2250... Loss: 1.7619... Val Loss: 1.7296\n",
            "Epoch: 21/50... Step: 2260... Loss: 1.7567... Val Loss: 1.7297\n",
            "Epoch: 21/50... Step: 2270... Loss: 1.7607... Val Loss: 1.7287\n",
            "Epoch: 21/50... Step: 2280... Loss: 1.7576... Val Loss: 1.7240\n",
            "Epoch: 21/50... Step: 2290... Loss: 1.7684... Val Loss: 1.7220\n",
            "Epoch: 21/50... Step: 2300... Loss: 1.7664... Val Loss: 1.7219\n",
            "Epoch: 21/50... Step: 2310... Loss: 1.7743... Val Loss: 1.7196\n",
            "Epoch: 21/50... Step: 2320... Loss: 1.7498... Val Loss: 1.7180\n",
            "Epoch: 21/50... Step: 2330... Loss: 1.7330... Val Loss: 1.7181\n",
            "Epoch: 21/50... Step: 2340... Loss: 1.7562... Val Loss: 1.7173\n",
            "Epoch: 21/50... Step: 2350... Loss: 1.7471... Val Loss: 1.7187\n",
            "Epoch: 22/50... Step: 2360... Loss: 1.7158... Val Loss: 1.7147\n",
            "Epoch: 22/50... Step: 2370... Loss: 1.7357... Val Loss: 1.7139\n",
            "Epoch: 22/50... Step: 2380... Loss: 1.7519... Val Loss: 1.7116\n",
            "Epoch: 22/50... Step: 2390... Loss: 1.7512... Val Loss: 1.7095\n",
            "Epoch: 22/50... Step: 2400... Loss: 1.7539... Val Loss: 1.7115\n",
            "Epoch: 22/50... Step: 2410... Loss: 1.7418... Val Loss: 1.7051\n",
            "Epoch: 22/50... Step: 2420... Loss: 1.7504... Val Loss: 1.7092\n",
            "Epoch: 22/50... Step: 2430... Loss: 1.7352... Val Loss: 1.7055\n",
            "Epoch: 22/50... Step: 2440... Loss: 1.7203... Val Loss: 1.7073\n",
            "Epoch: 22/50... Step: 2450... Loss: 1.7379... Val Loss: 1.7061\n",
            "Epoch: 22/50... Step: 2460... Loss: 1.7287... Val Loss: 1.7035\n",
            "Epoch: 23/50... Step: 2470... Loss: 1.7412... Val Loss: 1.7030\n",
            "Epoch: 23/50... Step: 2480... Loss: 1.7394... Val Loss: 1.7000\n",
            "Epoch: 23/50... Step: 2490... Loss: 1.7532... Val Loss: 1.6998\n",
            "Epoch: 23/50... Step: 2500... Loss: 1.7235... Val Loss: 1.6964\n",
            "Epoch: 23/50... Step: 2510... Loss: 1.7111... Val Loss: 1.6946\n",
            "Epoch: 23/50... Step: 2520... Loss: 1.7386... Val Loss: 1.6947\n",
            "Epoch: 23/50... Step: 2530... Loss: 1.7128... Val Loss: 1.6938\n",
            "Epoch: 23/50... Step: 2540... Loss: 1.7017... Val Loss: 1.6981\n",
            "Epoch: 23/50... Step: 2550... Loss: 1.7188... Val Loss: 1.6984\n",
            "Epoch: 23/50... Step: 2560... Loss: 1.7652... Val Loss: 1.7145\n",
            "Epoch: 23/50... Step: 2570... Loss: 1.7077... Val Loss: 1.7068\n",
            "Epoch: 24/50... Step: 2580... Loss: 1.7501... Val Loss: 1.6998\n",
            "Epoch: 24/50... Step: 2590... Loss: 1.7511... Val Loss: 1.6963\n",
            "Epoch: 24/50... Step: 2600... Loss: 1.7241... Val Loss: 1.6931\n",
            "Epoch: 24/50... Step: 2610... Loss: 1.7223... Val Loss: 1.6895\n",
            "Epoch: 24/50... Step: 2620... Loss: 1.7315... Val Loss: 1.6873\n",
            "Epoch: 24/50... Step: 2630... Loss: 1.7182... Val Loss: 1.6866\n",
            "Epoch: 24/50... Step: 2640... Loss: 1.7086... Val Loss: 1.6845\n",
            "Epoch: 24/50... Step: 2650... Loss: 1.7063... Val Loss: 1.6844\n",
            "Epoch: 24/50... Step: 2660... Loss: 1.7074... Val Loss: 1.6835\n",
            "Epoch: 24/50... Step: 2670... Loss: 1.7267... Val Loss: 1.6849\n",
            "Epoch: 24/50... Step: 2680... Loss: 1.7018... Val Loss: 1.6875\n",
            "Epoch: 25/50... Step: 2690... Loss: 1.7369... Val Loss: 1.6813\n",
            "Epoch: 25/50... Step: 2700... Loss: 1.6913... Val Loss: 1.6803\n",
            "Epoch: 25/50... Step: 2710... Loss: 1.6995... Val Loss: 1.6780\n",
            "Epoch: 25/50... Step: 2720... Loss: 1.7097... Val Loss: 1.6777\n",
            "Epoch: 25/50... Step: 2730... Loss: 1.6947... Val Loss: 1.6760\n",
            "Epoch: 25/50... Step: 2740... Loss: 1.6916... Val Loss: 1.6759\n",
            "Epoch: 25/50... Step: 2750... Loss: 1.6829... Val Loss: 1.6726\n",
            "Epoch: 25/50... Step: 2760... Loss: 1.6937... Val Loss: 1.6750\n",
            "Epoch: 25/50... Step: 2770... Loss: 1.6904... Val Loss: 1.6709\n",
            "Epoch: 25/50... Step: 2780... Loss: 1.6769... Val Loss: 1.6736\n",
            "Epoch: 25/50... Step: 2790... Loss: 1.6795... Val Loss: 1.6728\n",
            "Epoch: 25/50... Step: 2800... Loss: 1.6819... Val Loss: 1.6712\n",
            "Epoch: 26/50... Step: 2810... Loss: 1.6861... Val Loss: 1.6712\n",
            "Epoch: 26/50... Step: 2820... Loss: 1.6870... Val Loss: 1.6687\n",
            "Epoch: 26/50... Step: 2830... Loss: 1.6821... Val Loss: 1.6687\n",
            "Epoch: 26/50... Step: 2840... Loss: 1.6809... Val Loss: 1.6663\n",
            "Epoch: 26/50... Step: 2850... Loss: 1.7001... Val Loss: 1.6679\n",
            "Epoch: 26/50... Step: 2860... Loss: 1.6896... Val Loss: 1.6638\n",
            "Epoch: 26/50... Step: 2870... Loss: 1.6926... Val Loss: 1.6639\n",
            "Epoch: 26/50... Step: 2880... Loss: 1.6759... Val Loss: 1.6638\n",
            "Epoch: 26/50... Step: 2890... Loss: 1.6577... Val Loss: 1.6650\n",
            "Epoch: 26/50... Step: 2900... Loss: 1.6791... Val Loss: 1.6643\n",
            "Epoch: 26/50... Step: 2910... Loss: 1.6725... Val Loss: 1.6616\n",
            "Epoch: 27/50... Step: 2920... Loss: 1.6544... Val Loss: 1.6640\n",
            "Epoch: 27/50... Step: 2930... Loss: 1.6655... Val Loss: 1.6602\n",
            "Epoch: 27/50... Step: 2940... Loss: 1.6762... Val Loss: 1.6583\n",
            "Epoch: 27/50... Step: 2950... Loss: 1.6759... Val Loss: 1.6563\n",
            "Epoch: 27/50... Step: 2960... Loss: 1.6847... Val Loss: 1.6576\n",
            "Epoch: 27/50... Step: 2970... Loss: 1.6720... Val Loss: 1.6538\n",
            "Epoch: 27/50... Step: 2980... Loss: 1.6871... Val Loss: 1.6540\n",
            "Epoch: 27/50... Step: 2990... Loss: 1.6647... Val Loss: 1.6552\n",
            "Epoch: 27/50... Step: 3000... Loss: 1.6536... Val Loss: 1.6566\n",
            "Epoch: 27/50... Step: 3010... Loss: 1.6778... Val Loss: 1.6543\n",
            "Epoch: 27/50... Step: 3020... Loss: 1.6622... Val Loss: 1.6551\n",
            "Epoch: 28/50... Step: 3030... Loss: 1.6777... Val Loss: 1.6535\n",
            "Epoch: 28/50... Step: 3040... Loss: 1.6767... Val Loss: 1.6517\n",
            "Epoch: 28/50... Step: 3050... Loss: 1.6958... Val Loss: 1.6511\n",
            "Epoch: 28/50... Step: 3060... Loss: 1.6533... Val Loss: 1.6489\n",
            "Epoch: 28/50... Step: 3070... Loss: 1.6484... Val Loss: 1.6465\n",
            "Epoch: 28/50... Step: 3080... Loss: 1.6654... Val Loss: 1.6452\n",
            "Epoch: 28/50... Step: 3090... Loss: 1.6477... Val Loss: 1.6450\n",
            "Epoch: 28/50... Step: 3100... Loss: 1.6464... Val Loss: 1.6482\n",
            "Epoch: 28/50... Step: 3110... Loss: 1.6521... Val Loss: 1.6480\n",
            "Epoch: 28/50... Step: 3120... Loss: 1.6754... Val Loss: 1.6460\n",
            "Epoch: 28/50... Step: 3130... Loss: 1.6296... Val Loss: 1.6468\n",
            "Epoch: 29/50... Step: 3140... Loss: 1.6680... Val Loss: 1.6446\n",
            "Epoch: 29/50... Step: 3150... Loss: 1.6795... Val Loss: 1.6438\n",
            "Epoch: 29/50... Step: 3160... Loss: 1.6578... Val Loss: 1.6448\n",
            "Epoch: 29/50... Step: 3170... Loss: 1.6585... Val Loss: 1.6409\n",
            "Epoch: 29/50... Step: 3180... Loss: 1.6722... Val Loss: 1.6403\n",
            "Epoch: 29/50... Step: 3190... Loss: 1.6508... Val Loss: 1.6417\n",
            "Epoch: 29/50... Step: 3200... Loss: 1.6423... Val Loss: 1.6383\n",
            "Epoch: 29/50... Step: 3210... Loss: 1.6389... Val Loss: 1.6408\n",
            "Epoch: 29/50... Step: 3220... Loss: 1.6430... Val Loss: 1.6381\n",
            "Epoch: 29/50... Step: 3230... Loss: 1.6678... Val Loss: 1.6415\n",
            "Epoch: 29/50... Step: 3240... Loss: 1.6392... Val Loss: 1.6393\n",
            "Epoch: 30/50... Step: 3250... Loss: 1.6785... Val Loss: 1.6408\n",
            "Epoch: 30/50... Step: 3260... Loss: 1.6254... Val Loss: 1.6389\n",
            "Epoch: 30/50... Step: 3270... Loss: 1.6411... Val Loss: 1.6357\n",
            "Epoch: 30/50... Step: 3280... Loss: 1.6495... Val Loss: 1.6356\n",
            "Epoch: 30/50... Step: 3290... Loss: 1.6345... Val Loss: 1.6361\n",
            "Epoch: 30/50... Step: 3300... Loss: 1.6332... Val Loss: 1.6355\n",
            "Epoch: 30/50... Step: 3310... Loss: 1.6244... Val Loss: 1.6344\n",
            "Epoch: 30/50... Step: 3320... Loss: 1.6346... Val Loss: 1.6321\n",
            "Epoch: 30/50... Step: 3330... Loss: 1.6353... Val Loss: 1.6330\n",
            "Epoch: 30/50... Step: 3340... Loss: 1.6196... Val Loss: 1.6334\n",
            "Epoch: 30/50... Step: 3350... Loss: 1.6267... Val Loss: 1.6337\n",
            "Epoch: 30/50... Step: 3360... Loss: 1.6118... Val Loss: 1.6304\n",
            "Epoch: 31/50... Step: 3370... Loss: 1.6321... Val Loss: 1.6299\n",
            "Epoch: 31/50... Step: 3380... Loss: 1.6297... Val Loss: 1.6304\n",
            "Epoch: 31/50... Step: 3390... Loss: 1.6298... Val Loss: 1.6293\n",
            "Epoch: 31/50... Step: 3400... Loss: 1.6230... Val Loss: 1.6279\n",
            "Epoch: 31/50... Step: 3410... Loss: 1.6382... Val Loss: 1.6278\n",
            "Epoch: 31/50... Step: 3420... Loss: 1.6355... Val Loss: 1.6253\n",
            "Epoch: 31/50... Step: 3430... Loss: 1.6421... Val Loss: 1.6286\n",
            "Epoch: 31/50... Step: 3440... Loss: 1.6293... Val Loss: 1.6283\n",
            "Epoch: 31/50... Step: 3450... Loss: 1.6040... Val Loss: 1.6296\n",
            "Epoch: 31/50... Step: 3460... Loss: 1.6255... Val Loss: 1.6274\n",
            "Epoch: 31/50... Step: 3470... Loss: 1.6209... Val Loss: 1.6254\n",
            "Epoch: 32/50... Step: 3480... Loss: 1.5915... Val Loss: 1.6265\n",
            "Epoch: 32/50... Step: 3490... Loss: 1.6126... Val Loss: 1.6231\n",
            "Epoch: 32/50... Step: 3500... Loss: 1.6193... Val Loss: 1.6244\n",
            "Epoch: 32/50... Step: 3510... Loss: 1.6309... Val Loss: 1.6228\n",
            "Epoch: 32/50... Step: 3520... Loss: 1.6319... Val Loss: 1.6226\n",
            "Epoch: 32/50... Step: 3530... Loss: 1.6266... Val Loss: 1.6197\n",
            "Epoch: 32/50... Step: 3540... Loss: 1.6293... Val Loss: 1.6187\n",
            "Epoch: 32/50... Step: 3550... Loss: 1.6172... Val Loss: 1.6224\n",
            "Epoch: 32/50... Step: 3560... Loss: 1.6017... Val Loss: 1.6251\n",
            "Epoch: 32/50... Step: 3570... Loss: 1.6132... Val Loss: 1.6215\n",
            "Epoch: 32/50... Step: 3580... Loss: 1.6187... Val Loss: 1.6201\n",
            "Epoch: 33/50... Step: 3590... Loss: 1.6263... Val Loss: 1.6204\n",
            "Epoch: 33/50... Step: 3600... Loss: 1.6245... Val Loss: 1.6176\n",
            "Epoch: 33/50... Step: 3610... Loss: 1.6423... Val Loss: 1.6197\n",
            "Epoch: 33/50... Step: 3620... Loss: 1.6074... Val Loss: 1.6192\n",
            "Epoch: 33/50... Step: 3630... Loss: 1.5990... Val Loss: 1.6171\n",
            "Epoch: 33/50... Step: 3640... Loss: 1.6254... Val Loss: 1.6158\n",
            "Epoch: 33/50... Step: 3650... Loss: 1.6011... Val Loss: 1.6157\n",
            "Epoch: 33/50... Step: 3660... Loss: 1.5937... Val Loss: 1.6193\n",
            "Epoch: 33/50... Step: 3670... Loss: 1.6108... Val Loss: 1.6166\n",
            "Epoch: 33/50... Step: 3680... Loss: 1.6266... Val Loss: 1.6159\n",
            "Epoch: 33/50... Step: 3690... Loss: 1.5871... Val Loss: 1.6193\n",
            "Epoch: 34/50... Step: 3700... Loss: 1.6233... Val Loss: 1.6170\n",
            "Epoch: 34/50... Step: 3710... Loss: 1.6309... Val Loss: 1.6167\n",
            "Epoch: 34/50... Step: 3720... Loss: 1.5980... Val Loss: 1.6155\n",
            "Epoch: 34/50... Step: 3730... Loss: 1.6072... Val Loss: 1.6127\n",
            "Epoch: 34/50... Step: 3740... Loss: 1.6129... Val Loss: 1.6140\n",
            "Epoch: 34/50... Step: 3750... Loss: 1.6070... Val Loss: 1.6129\n",
            "Epoch: 34/50... Step: 3760... Loss: 1.6008... Val Loss: 1.6133\n",
            "Epoch: 34/50... Step: 3770... Loss: 1.5925... Val Loss: 1.6110\n",
            "Epoch: 34/50... Step: 3780... Loss: 1.6034... Val Loss: 1.6093\n",
            "Epoch: 34/50... Step: 3790... Loss: 1.6170... Val Loss: 1.6125\n",
            "Epoch: 34/50... Step: 3800... Loss: 1.5912... Val Loss: 1.6146\n",
            "Epoch: 35/50... Step: 3810... Loss: 1.6429... Val Loss: 1.6130\n",
            "Epoch: 35/50... Step: 3820... Loss: 1.5769... Val Loss: 1.6114\n",
            "Epoch: 35/50... Step: 3830... Loss: 1.5973... Val Loss: 1.6096\n",
            "Epoch: 35/50... Step: 3840... Loss: 1.6071... Val Loss: 1.6082\n",
            "Epoch: 35/50... Step: 3850... Loss: 1.5869... Val Loss: 1.6088\n",
            "Epoch: 35/50... Step: 3860... Loss: 1.5979... Val Loss: 1.6064\n",
            "Epoch: 35/50... Step: 3870... Loss: 1.5883... Val Loss: 1.6070\n",
            "Epoch: 35/50... Step: 3880... Loss: 1.5862... Val Loss: 1.6068\n",
            "Epoch: 35/50... Step: 3890... Loss: 1.5955... Val Loss: 1.6056\n",
            "Epoch: 35/50... Step: 3900... Loss: 1.5770... Val Loss: 1.6082\n",
            "Epoch: 35/50... Step: 3910... Loss: 1.5815... Val Loss: 1.6087\n",
            "Epoch: 35/50... Step: 3920... Loss: 1.5717... Val Loss: 1.6079\n",
            "Epoch: 36/50... Step: 3930... Loss: 1.5962... Val Loss: 1.6072\n",
            "Epoch: 36/50... Step: 3940... Loss: 1.5960... Val Loss: 1.6053\n",
            "Epoch: 36/50... Step: 3950... Loss: 1.5851... Val Loss: 1.6070\n",
            "Epoch: 36/50... Step: 3960... Loss: 1.5881... Val Loss: 1.6071\n",
            "Epoch: 36/50... Step: 3970... Loss: 1.6039... Val Loss: 1.6059\n",
            "Epoch: 36/50... Step: 3980... Loss: 1.5902... Val Loss: 1.6005\n",
            "Epoch: 36/50... Step: 3990... Loss: 1.6058... Val Loss: 1.6008\n",
            "Epoch: 36/50... Step: 4000... Loss: 1.5848... Val Loss: 1.6032\n",
            "Epoch: 36/50... Step: 4010... Loss: 1.5623... Val Loss: 1.6074\n",
            "Epoch: 36/50... Step: 4020... Loss: 1.5813... Val Loss: 1.6051\n",
            "Epoch: 36/50... Step: 4030... Loss: 1.5823... Val Loss: 1.6028\n",
            "Epoch: 37/50... Step: 4040... Loss: 1.5565... Val Loss: 1.6018\n",
            "Epoch: 37/50... Step: 4050... Loss: 1.5724... Val Loss: 1.6017\n",
            "Epoch: 37/50... Step: 4060... Loss: 1.5845... Val Loss: 1.6007\n",
            "Epoch: 37/50... Step: 4070... Loss: 1.5971... Val Loss: 1.6017\n",
            "Epoch: 37/50... Step: 4080... Loss: 1.5967... Val Loss: 1.5999\n",
            "Epoch: 37/50... Step: 4090... Loss: 1.5882... Val Loss: 1.5977\n",
            "Epoch: 37/50... Step: 4100... Loss: 1.5927... Val Loss: 1.5982\n",
            "Epoch: 37/50... Step: 4110... Loss: 1.5807... Val Loss: 1.6015\n",
            "Epoch: 37/50... Step: 4120... Loss: 1.5637... Val Loss: 1.5987\n",
            "Epoch: 37/50... Step: 4130... Loss: 1.5800... Val Loss: 1.6020\n",
            "Epoch: 37/50... Step: 4140... Loss: 1.5750... Val Loss: 1.6031\n",
            "Epoch: 38/50... Step: 4150... Loss: 1.5913... Val Loss: 1.6000\n",
            "Epoch: 38/50... Step: 4160... Loss: 1.5903... Val Loss: 1.5981\n",
            "Epoch: 38/50... Step: 4170... Loss: 1.5971... Val Loss: 1.5977\n",
            "Epoch: 38/50... Step: 4180... Loss: 1.5685... Val Loss: 1.5987\n",
            "Epoch: 38/50... Step: 4190... Loss: 1.5593... Val Loss: 1.5971\n",
            "Epoch: 38/50... Step: 4200... Loss: 1.5793... Val Loss: 1.5970\n",
            "Epoch: 38/50... Step: 4210... Loss: 1.5569... Val Loss: 1.5984\n",
            "Epoch: 38/50... Step: 4220... Loss: 1.5602... Val Loss: 1.5978\n",
            "Epoch: 38/50... Step: 4230... Loss: 1.5705... Val Loss: 1.5983\n",
            "Epoch: 38/50... Step: 4240... Loss: 1.5829... Val Loss: 1.5998\n",
            "Epoch: 38/50... Step: 4250... Loss: 1.5459... Val Loss: 1.6004\n",
            "Epoch: 39/50... Step: 4260... Loss: 1.5902... Val Loss: 1.5953\n",
            "Epoch: 39/50... Step: 4270... Loss: 1.5983... Val Loss: 1.5935\n",
            "Epoch: 39/50... Step: 4280... Loss: 1.5634... Val Loss: 1.5963\n",
            "Epoch: 39/50... Step: 4290... Loss: 1.5744... Val Loss: 1.5944\n",
            "Epoch: 39/50... Step: 4300... Loss: 1.5763... Val Loss: 1.5984\n",
            "Epoch: 39/50... Step: 4310... Loss: 1.5665... Val Loss: 1.5953\n",
            "Epoch: 39/50... Step: 4320... Loss: 1.5594... Val Loss: 1.5944\n",
            "Epoch: 39/50... Step: 4330... Loss: 1.5590... Val Loss: 1.5923\n",
            "Epoch: 39/50... Step: 4340... Loss: 1.5665... Val Loss: 1.5917\n",
            "Epoch: 39/50... Step: 4350... Loss: 1.5835... Val Loss: 1.5938\n",
            "Epoch: 39/50... Step: 4360... Loss: 1.5582... Val Loss: 1.5960\n",
            "Epoch: 40/50... Step: 4370... Loss: 1.6007... Val Loss: 1.5920\n",
            "Epoch: 40/50... Step: 4380... Loss: 1.5519... Val Loss: 1.5930\n",
            "Epoch: 40/50... Step: 4390... Loss: 1.5655... Val Loss: 1.5922\n",
            "Epoch: 40/50... Step: 4400... Loss: 1.5658... Val Loss: 1.5920\n",
            "Epoch: 40/50... Step: 4410... Loss: 1.5601... Val Loss: 1.5921\n",
            "Epoch: 40/50... Step: 4420... Loss: 1.5636... Val Loss: 1.5928\n",
            "Epoch: 40/50... Step: 4430... Loss: 1.5493... Val Loss: 1.5878\n",
            "Epoch: 40/50... Step: 4440... Loss: 1.5530... Val Loss: 1.5891\n",
            "Epoch: 40/50... Step: 4450... Loss: 1.5537... Val Loss: 1.5921\n",
            "Epoch: 40/50... Step: 4460... Loss: 1.5403... Val Loss: 1.5935\n",
            "Epoch: 40/50... Step: 4470... Loss: 1.5528... Val Loss: 1.5935\n",
            "Epoch: 40/50... Step: 4480... Loss: 1.5392... Val Loss: 1.5906\n",
            "Epoch: 41/50... Step: 4490... Loss: 1.5501... Val Loss: 1.5943\n",
            "Epoch: 41/50... Step: 4500... Loss: 1.5535... Val Loss: 1.5921\n",
            "Epoch: 41/50... Step: 4510... Loss: 1.5533... Val Loss: 1.5904\n",
            "Epoch: 41/50... Step: 4520... Loss: 1.5577... Val Loss: 1.5902\n",
            "Epoch: 41/50... Step: 4530... Loss: 1.5633... Val Loss: 1.5911\n",
            "Epoch: 41/50... Step: 4540... Loss: 1.5590... Val Loss: 1.5869\n",
            "Epoch: 41/50... Step: 4550... Loss: 1.5724... Val Loss: 1.5871\n",
            "Epoch: 41/50... Step: 4560... Loss: 1.5491... Val Loss: 1.5899\n",
            "Epoch: 41/50... Step: 4570... Loss: 1.5288... Val Loss: 1.5890\n",
            "Epoch: 41/50... Step: 4580... Loss: 1.5463... Val Loss: 1.5890\n",
            "Epoch: 41/50... Step: 4590... Loss: 1.5534... Val Loss: 1.5902\n",
            "Epoch: 42/50... Step: 4600... Loss: 1.5231... Val Loss: 1.5906\n",
            "Epoch: 42/50... Step: 4610... Loss: 1.5409... Val Loss: 1.5898\n",
            "Epoch: 42/50... Step: 4620... Loss: 1.5509... Val Loss: 1.5898\n",
            "Epoch: 42/50... Step: 4630... Loss: 1.5542... Val Loss: 1.5886\n",
            "Epoch: 42/50... Step: 4640... Loss: 1.5614... Val Loss: 1.5878\n",
            "Epoch: 42/50... Step: 4650... Loss: 1.5577... Val Loss: 1.5860\n",
            "Epoch: 42/50... Step: 4660... Loss: 1.5550... Val Loss: 1.5855\n",
            "Epoch: 42/50... Step: 4670... Loss: 1.5515... Val Loss: 1.5874\n",
            "Epoch: 42/50... Step: 4680... Loss: 1.5281... Val Loss: 1.5861\n",
            "Epoch: 42/50... Step: 4690... Loss: 1.5473... Val Loss: 1.5880\n",
            "Epoch: 42/50... Step: 4700... Loss: 1.5388... Val Loss: 1.5914\n",
            "Epoch: 43/50... Step: 4710... Loss: 1.5598... Val Loss: 1.5857\n",
            "Epoch: 43/50... Step: 4720... Loss: 1.5588... Val Loss: 1.5851\n",
            "Epoch: 43/50... Step: 4730... Loss: 1.5735... Val Loss: 1.5884\n",
            "Epoch: 43/50... Step: 4740... Loss: 1.5353... Val Loss: 1.5851\n",
            "Epoch: 43/50... Step: 4750... Loss: 1.5258... Val Loss: 1.5871\n",
            "Epoch: 43/50... Step: 4760... Loss: 1.5534... Val Loss: 1.5853\n",
            "Epoch: 43/50... Step: 4770... Loss: 1.5272... Val Loss: 1.5855\n",
            "Epoch: 43/50... Step: 4780... Loss: 1.5346... Val Loss: 1.5849\n",
            "Epoch: 43/50... Step: 4790... Loss: 1.5420... Val Loss: 1.5843\n",
            "Epoch: 43/50... Step: 4800... Loss: 1.5489... Val Loss: 1.5837\n",
            "Epoch: 43/50... Step: 4810... Loss: 1.5209... Val Loss: 1.5887\n",
            "Epoch: 44/50... Step: 4820... Loss: 1.5556... Val Loss: 1.5822\n",
            "Epoch: 44/50... Step: 4830... Loss: 1.5609... Val Loss: 1.5827\n",
            "Epoch: 44/50... Step: 4840... Loss: 1.5334... Val Loss: 1.5861\n",
            "Epoch: 44/50... Step: 4850... Loss: 1.5409... Val Loss: 1.5850\n",
            "Epoch: 44/50... Step: 4860... Loss: 1.5428... Val Loss: 1.5859\n",
            "Epoch: 44/50... Step: 4870... Loss: 1.5381... Val Loss: 1.5858\n",
            "Epoch: 44/50... Step: 4880... Loss: 1.5259... Val Loss: 1.5807\n",
            "Epoch: 44/50... Step: 4890... Loss: 1.5284... Val Loss: 1.5820\n",
            "Epoch: 44/50... Step: 4900... Loss: 1.5378... Val Loss: 1.5806\n",
            "Epoch: 44/50... Step: 4910... Loss: 1.5552... Val Loss: 1.5832\n",
            "Epoch: 44/50... Step: 4920... Loss: 1.5324... Val Loss: 1.5826\n",
            "Epoch: 45/50... Step: 4930... Loss: 1.5736... Val Loss: 1.5817\n",
            "Epoch: 45/50... Step: 4940... Loss: 1.5201... Val Loss: 1.5852\n",
            "Epoch: 45/50... Step: 4950... Loss: 1.5340... Val Loss: 1.5825\n",
            "Epoch: 45/50... Step: 4960... Loss: 1.5472... Val Loss: 1.5805\n",
            "Epoch: 45/50... Step: 4970... Loss: 1.5254... Val Loss: 1.5800\n",
            "Epoch: 45/50... Step: 4980... Loss: 1.5306... Val Loss: 1.5785\n",
            "Epoch: 45/50... Step: 4990... Loss: 1.5273... Val Loss: 1.5762\n",
            "Epoch: 45/50... Step: 5000... Loss: 1.5190... Val Loss: 1.5799\n",
            "Epoch: 45/50... Step: 5010... Loss: 1.5327... Val Loss: 1.5832\n",
            "Epoch: 45/50... Step: 5020... Loss: 1.5132... Val Loss: 1.5848\n",
            "Epoch: 45/50... Step: 5030... Loss: 1.5231... Val Loss: 1.5823\n",
            "Epoch: 45/50... Step: 5040... Loss: 1.5082... Val Loss: 1.5823\n",
            "Epoch: 46/50... Step: 5050... Loss: 1.5313... Val Loss: 1.5849\n",
            "Epoch: 46/50... Step: 5060... Loss: 1.5281... Val Loss: 1.5842\n",
            "Epoch: 46/50... Step: 5070... Loss: 1.5299... Val Loss: 1.5808\n",
            "Epoch: 46/50... Step: 5080... Loss: 1.5294... Val Loss: 1.5784\n",
            "Epoch: 46/50... Step: 5090... Loss: 1.5402... Val Loss: 1.5821\n",
            "Epoch: 46/50... Step: 5100... Loss: 1.5243... Val Loss: 1.5827\n",
            "Epoch: 46/50... Step: 5110... Loss: 1.5427... Val Loss: 1.5804\n",
            "Epoch: 46/50... Step: 5120... Loss: 1.5240... Val Loss: 1.5828\n",
            "Epoch: 46/50... Step: 5130... Loss: 1.5029... Val Loss: 1.5796\n",
            "Epoch: 46/50... Step: 5140... Loss: 1.5277... Val Loss: 1.5802\n",
            "Epoch: 46/50... Step: 5150... Loss: 1.5227... Val Loss: 1.5829\n",
            "Epoch: 47/50... Step: 5160... Loss: 1.4988... Val Loss: 1.5803\n",
            "Epoch: 47/50... Step: 5170... Loss: 1.5139... Val Loss: 1.5790\n",
            "Epoch: 47/50... Step: 5180... Loss: 1.5241... Val Loss: 1.5806\n",
            "Epoch: 47/50... Step: 5190... Loss: 1.5247... Val Loss: 1.5789\n",
            "Epoch: 47/50... Step: 5200... Loss: 1.5286... Val Loss: 1.5792\n",
            "Epoch: 47/50... Step: 5210... Loss: 1.5294... Val Loss: 1.5769\n",
            "Epoch: 47/50... Step: 5220... Loss: 1.5305... Val Loss: 1.5772\n",
            "Epoch: 47/50... Step: 5230... Loss: 1.5208... Val Loss: 1.5796\n",
            "Epoch: 47/50... Step: 5240... Loss: 1.5005... Val Loss: 1.5771\n",
            "Epoch: 47/50... Step: 5250... Loss: 1.5196... Val Loss: 1.5798\n",
            "Epoch: 47/50... Step: 5260... Loss: 1.5146... Val Loss: 1.5823\n",
            "Epoch: 48/50... Step: 5270... Loss: 1.5334... Val Loss: 1.5770\n",
            "Epoch: 48/50... Step: 5280... Loss: 1.5355... Val Loss: 1.5764\n",
            "Epoch: 48/50... Step: 5290... Loss: 1.5419... Val Loss: 1.5813\n",
            "Epoch: 48/50... Step: 5300... Loss: 1.5057... Val Loss: 1.5792\n",
            "Epoch: 48/50... Step: 5310... Loss: 1.5032... Val Loss: 1.5800\n",
            "Epoch: 48/50... Step: 5320... Loss: 1.5214... Val Loss: 1.5778\n",
            "Epoch: 48/50... Step: 5330... Loss: 1.5140... Val Loss: 1.5746\n",
            "Epoch: 48/50... Step: 5340... Loss: 1.5083... Val Loss: 1.5758\n",
            "Epoch: 48/50... Step: 5350... Loss: 1.5141... Val Loss: 1.5760\n",
            "Epoch: 48/50... Step: 5360... Loss: 1.5335... Val Loss: 1.5754\n",
            "Epoch: 48/50... Step: 5370... Loss: 1.4977... Val Loss: 1.5814\n",
            "Epoch: 49/50... Step: 5380... Loss: 1.5286... Val Loss: 1.5769\n",
            "Epoch: 49/50... Step: 5390... Loss: 1.5266... Val Loss: 1.5768\n",
            "Epoch: 49/50... Step: 5400... Loss: 1.5076... Val Loss: 1.5777\n",
            "Epoch: 49/50... Step: 5410... Loss: 1.5203... Val Loss: 1.5757\n",
            "Epoch: 49/50... Step: 5420... Loss: 1.5248... Val Loss: 1.5750\n",
            "Epoch: 49/50... Step: 5430... Loss: 1.5081... Val Loss: 1.5772\n",
            "Epoch: 49/50... Step: 5440... Loss: 1.5024... Val Loss: 1.5750\n",
            "Epoch: 49/50... Step: 5450... Loss: 1.5055... Val Loss: 1.5743\n",
            "Epoch: 49/50... Step: 5460... Loss: 1.5121... Val Loss: 1.5744\n",
            "Epoch: 49/50... Step: 5470... Loss: 1.5273... Val Loss: 1.5796\n",
            "Epoch: 49/50... Step: 5480... Loss: 1.5124... Val Loss: 1.5777\n",
            "Epoch: 50/50... Step: 5490... Loss: 1.5484... Val Loss: 1.5774\n",
            "Epoch: 50/50... Step: 5500... Loss: 1.4912... Val Loss: 1.5795\n",
            "Epoch: 50/50... Step: 5510... Loss: 1.5144... Val Loss: 1.5787\n",
            "Epoch: 50/50... Step: 5520... Loss: 1.5132... Val Loss: 1.5748\n",
            "Epoch: 50/50... Step: 5530... Loss: 1.5093... Val Loss: 1.5764\n",
            "Epoch: 50/50... Step: 5540... Loss: 1.5079... Val Loss: 1.5765\n",
            "Epoch: 50/50... Step: 5550... Loss: 1.5010... Val Loss: 1.5747\n",
            "Epoch: 50/50... Step: 5560... Loss: 1.5012... Val Loss: 1.5768\n",
            "Epoch: 50/50... Step: 5570... Loss: 1.5118... Val Loss: 1.5783\n",
            "Epoch: 50/50... Step: 5580... Loss: 1.4880... Val Loss: 1.5786\n",
            "Epoch: 50/50... Step: 5590... Loss: 1.4982... Val Loss: 1.5774\n",
            "Epoch: 50/50... Step: 5600... Loss: 1.4906... Val Loss: 1.5771\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "seq_length = 200\n",
        "n_epochs = 50\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfZxvNoDceEm"
      },
      "source": [
        "## Checkpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6RXl5VAceEm"
      },
      "outputs": [],
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = \"char_lstm_50_epoch.net\"\n",
        "\n",
        "# checkpoint = {\n",
        "#     \"n_hidden\": net.n_hidden,\n",
        "#     \"n_layers\": net.n_layers,\n",
        "#     \"state_dict\": net.state_dict(),\n",
        "#     \"tokens\": net.chars,\n",
        "# }\n",
        "\n",
        "# with open(model_name, \"wb\") as f:\n",
        "#     torch.save(checkpoint, f)\n",
        "\n",
        "torch.save(net, model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2sJhx5iceEm"
      },
      "source": [
        "---\n",
        "## Делаем предсказания\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEIRW_B2ceEm"
      },
      "outputs": [],
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG38j3gQceEm"
      },
      "source": [
        "### Priming и генерирование текста\n",
        "\n",
        "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9vpB5gRceEm"
      },
      "outputs": [],
      "source": [
        "def sample(net, size, prime=\"Инканденца\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqmFA9eEceEm",
        "outputId": "b78b6b0f-8d03-454e-cbd6-d3aa449e94cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Орин Инканденца выстреивает с поднятой собственной спинкой в положении и со своими поднятом столовых сторон и после простора в полу при столовом подобной противорезивов с поднятыми половенками и приверживыми состояниями положения сталового подставка и стало пробежеть, и подозревали, когда она, при этом просто происходить, как он не просто принимает при этом по середине собственного поднятая соседняя сторона старой проблемы в подобных полевениях в положении, как признаки собравая положительное стекло, когда просто после подняли столько под подбородком и проблемам в картридже в присутствии из-за того, что под столом поднял под просто под простой приставной стекло приступи в подскочке и проблема при себе в получение привычка в получерия по себе в просторном состоянии, пока не собирается, когда под подном стальной подозрения в получении проблемах и старших стенах, и просто поднимает под собой, пока не поднимается на себя в полу под простой полотенце и поднял себе в компании и после просторных составов в п\n"
          ]
        }
      ],
      "source": [
        "print(sample(net, 1000, prime=\"Орин Инканденца\", top_k=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "942mjdQHceEm"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhKGVUj2k2Mo"
      },
      "source": [
        "Цитата из текста: 'Талант сам по себе ожидание... либо оправдываешь его, либо он машет платочком, исчезая навсегда.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-qnXN8N6E6W"
      },
      "outputs": [],
      "source": [
        "loaded = torch.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut6R3zDcceEm",
        "outputId": "b5e7fa8c-c150-4642-eff5-9aa9306afdcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Талант сам по себе ожидание, никто не заметил. Последний стекленная концепта своего вопроса просмотра стоика из них, пока все все это со всей предметы просто переживающего сексуальное кориченио и продвигаться по картреджам на коленях и сообщает высокий корт.\n",
            "\n",
            "– Посмотреть про себя и стало себе, и ты воспринимаешь.\n",
            "\n",
            "– Но не ступает на комнате по поведении выражения.\n",
            "\n",
            "– Не обремо просто не знаешь, то еще приставить, наверное, не сказал он, вот так и вы пришлось.\n",
            "\n",
            "– Призрак?\n",
            "\n",
            "– Я ты дейся и никогда, в стене, – она показывает, что все еще не помогла на проблему историю при этом образи, когда показал столок, – совершенно высокая последний матчей на паре, который всегда все стоял на семейку на секундой в своей стороне классического передавления.\n",
            "\n",
            "– Только по твоему тень, когда она сказала:\n",
            "\n",
            "– Я поднимаюсь в трезвом степени. Но не опускался стен и поднимающийся в круглие, чтобы он последние сложные представьем корте, когда в том, что ты на востоке со всей виней. Он не выбирают не столько, но прямо вопль не столько.\n",
            "\n",
            "– В\n"
          ]
        }
      ],
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 1000, top_k=5, prime=\"Талант сам по себе ожидание\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHtthF0BI1RO"
      },
      "source": [
        "# Word-Level LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgeBZ8sJCuN"
      },
      "source": [
        "## Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNs1iVMZC3s8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZiIvJrBcC8ne",
        "outputId": "f889dc36-2ec9-444b-a6c8-1ad6526adbc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Бесконечная шутка\\nДэвид Фостер Уоллес\\n\\n\\nВеликие романы\\nВ недалеком будущем пациенты реабилитационной'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82nMjOO1JEtK",
        "outputId": "8324265a-c8fc-47d5-8e64-b6d58a054845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117706\n",
            "117706\n",
            "Дэвид\n"
          ]
        }
      ],
      "source": [
        "# Токенизируем слова\n",
        "# Разделяем текст на\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "vocab = list(word_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "SEQUENCE_LENGTH = 64\n",
        "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n",
        "print(len(vocab))\n",
        "print(len(word_to_int))\n",
        "print(int_to_word[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWlHPlVoJMOh"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XApGjnBDJOtb"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, samples, word_to_int):\n",
        "        self.samples = samples\n",
        "        self.word_to_int = word_to_int\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
        "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
        "        return input_seq, target_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdF1B42qJUs1",
        "outputId": "7c0c2b9d-b5c6-409a-e7d1-97f9a2cf8f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
            "        19, 20, 21, 22, 14, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
            "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
            "        54, 55, 14, 56, 57, 58, 59, 60, 61, 62]), tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
            "        20, 21, 22, 14, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
            "        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
            "        55, 14, 56, 57, 58, 59, 60, 61, 62, 63]))\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = TextDataset(samples, word_to_int)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "print(dataset[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyVHSZrpJYWa"
      },
      "outputs": [],
      "source": [
        "class TextGenerationLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_size,\n",
        "        num_layers\n",
        "    ):\n",
        "        super(TextGenerationLSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # nn.init.xavier_uniform_(self.lstm.weight)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        if hidden == None:\n",
        "            hidden = self.init_hidden(x.shape[0])\n",
        "        x = self.embedding(x)\n",
        "        out, (h_n, c_n) = self.lstm(x, hidden)\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, (h_n, c_n)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return h0, c0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GFn2rz0Jt3w"
      },
      "source": [
        "## Установим гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-_sh96BJe2e"
      },
      "outputs": [],
      "source": [
        "# Training Setup\n",
        "embedding_dim = 32\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "learning_rate = 0.01\n",
        "epochs = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFO7uIuBJg2F"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextGenerationLSTM(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    hidden_size,\n",
        "    num_layers\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7xh4WUJpTe"
      },
      "source": [
        "## Обучим модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZqBEnAtJixz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2982a0d0-d4be-4d8d-ca04-3d84ff6c1cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 2.465\n",
            "Epoch 1 loss: 1.355\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "def train(model, epochs, dataloader, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "            outputs, _ = model(input_seq)\n",
        "            loss = criterion(outputs, target_seq.view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.detach().cpu().numpy()\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")\n",
        "train(model, epochs, dataloader, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnddYnexJoft"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BGOynuHJ7k9"
      },
      "source": [
        "## Делаем предсказания"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0li8wL2dKGFv"
      },
      "outputs": [],
      "source": [
        "model_name = 'word_lstm_5_epoch.net'\n",
        "torch.save(model, model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPsOIDj7KiZj"
      },
      "outputs": [],
      "source": [
        "loaded_word = torch.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53dljl7hJ6-g"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_words):\n",
        "    model.eval()\n",
        "    words = start_string.split()\n",
        "    for _ in range(num_words):\n",
        "        input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0).to(device)\n",
        "        h, c = model.init_hidden(1)\n",
        "        output, (h, c) = model(input_seq, (h, c))\n",
        "        next_token = output.argmax(1)[-1].item()\n",
        "        words.append(int_to_word[next_token])\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Example usage:\n",
        "print(\"Generated Text:\", generate_text(loaded_word, start_string=\"Талант сам по себе\", num_words=1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tePY0txNfBe7"
      },
      "source": [
        "**Выводы:**\n",
        "\n",
        "- Построены две модели разных подходов: посимвольной генерации и пословной.\n",
        "- Метрики моделей достаточно близки\n",
        "- Конечный результат далёк от идеального. Возможно это из-за выбранного текста: объёмный + большое количество слов придумано автором + в данном случае видим мы считываем перевод изначально достаточно сложного текста, который принципиально непереводим. Так что возможно это повлияло на результат. Либо возможно следовало усложнить модели."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}