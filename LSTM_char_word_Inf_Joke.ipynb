{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "LSTM_char_word_Inf_Joke",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7594846,
          "sourceType": "datasetVersion",
          "datasetId": 4420678
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Text_Gen_RNN/blob/model/LSTM_char_word_Inf_Joke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'infinite-joke-txt:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4420678%2F7594846%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240322%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240322T115527Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5d35099242f645ef1ee9aa4609f30ac2e0ffb7ac06f732f5d703170ab9deb0a552b30fa0d3eea8d6d01bd5fdbdf88284f5c752c09a6c350b666cc54adc8ac65e85e59917015bab9e9dab12907f982e9072dfb172f5607bd80e5b77cb8e32b5d2161016b67bc7d6bba7a150aea139d4a20c73f11261dbda9a294d9874d9c57df8b2a08f3196c97bd6003c3107aacbbd70d3b6bab0282d828338e456d1ebec8d298aeaba202bf99d1a2522073364734a9b620e1eb522c1328cdb57ec49e829bbfee11040b1d39333a290af98838a9d5fd47c0be28dbb0137b4f6e58bd396157472e00be2eb2be31265f6c796eb5cd7b59aefcb9e9acba8baf919764a58828549bb'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "BogNdMS0BChe"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character-Level LSTM\n",
        "Обучим модель на тексте книги \"Бесконечная шутка\", после чего попробуем генерировать новый текст."
      ],
      "metadata": {
        "id": "1W8R8WgZceEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "sqUOE2flceEl",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:20.57712Z",
          "iopub.execute_input": "2024-03-22T09:25:20.577492Z",
          "iopub.status.idle": "2024-03-22T09:25:27.612571Z",
          "shell.execute_reply.started": "2024-03-22T09:25:20.577461Z",
          "shell.execute_reply": "2024-03-22T09:25:27.611525Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# фиксируем воспроизводимость\n",
        "def seed_everything(seed=42):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed=7575)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:27.614465Z",
          "iopub.execute_input": "2024-03-22T09:25:27.614944Z",
          "iopub.status.idle": "2024-03-22T09:25:27.624714Z",
          "shell.execute_reply.started": "2024-03-22T09:25:27.614902Z",
          "shell.execute_reply": "2024-03-22T09:25:27.623615Z"
        },
        "trusted": true,
        "id": "QJYp0PjEBChj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузим данные\n"
      ],
      "metadata": {
        "id": "_wHfCDyzceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # open text file and read in data as `text`\n",
        "# with open(\"/content/Uolles_D._Velikieromanyi._Beskonechnaya_Shutka.txt\", \"r\") as f:\n",
        "#     text = f.read()\n",
        "\n",
        "with open(\"/kaggle/input/infinite-joke-txt/Uolles_D._Velikieromanyi._Beskonechnaya_Shutka.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "b34kfqIOceEl",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:27.626488Z",
          "iopub.execute_input": "2024-03-22T09:25:27.626858Z",
          "iopub.status.idle": "2024-03-22T09:25:27.830027Z",
          "shell.execute_reply.started": "2024-03-22T09:25:27.626805Z",
          "shell.execute_reply": "2024-03-22T09:25:27.829134Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим первые 100 символов:"
      ],
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[:108]"
      ],
      "metadata": {
        "id": "7VctmLQfceEl",
        "outputId": "4276bf88-2d56-4165-8cdb-b591ba66e15c",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:27.832564Z",
          "iopub.execute_input": "2024-03-22T09:25:27.832849Z",
          "iopub.status.idle": "2024-03-22T09:25:27.840027Z",
          "shell.execute_reply.started": "2024-03-22T09:25:27.832824Z",
          "shell.execute_reply": "2024-03-22T09:25:27.839043Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Бесконечная шутка\\nДэвид Фостер Уоллес\\n\\n\\nВеликие романы\\nВ недалеком будущем пациенты реабилитационной клиники'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация\n",
        "\n",
        "В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
      ],
      "metadata": {
        "id": "4iC21bopceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "tYVlmnxLceEl",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:27.841197Z",
          "iopub.execute_input": "2024-03-22T09:25:27.84149Z",
          "iopub.status.idle": "2024-03-22T09:25:28.749565Z",
          "shell.execute_reply.started": "2024-03-22T09:25:27.841466Z",
          "shell.execute_reply": "2024-03-22T09:25:28.74845Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим как символы закодировались целыми числами"
      ],
      "metadata": {
        "id": "oJIzwzSwceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "metadata": {
        "id": "WK1MYr_9ceEl",
        "outputId": "207138ed-b941-4b33-da33-35a5cc3f77dc",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.75082Z",
          "iopub.execute_input": "2024-03-22T09:25:28.751116Z",
          "iopub.status.idle": "2024-03-22T09:25:28.758583Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.751091Z",
          "shell.execute_reply": "2024-03-22T09:25:28.757411Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([ 46,   9,  94, 133, 129, 157,   9,  80, 157, 158, 148, 100, 147,\n        68,  55, 133, 158, 161, 110,  57,  50,  56, 164, 100, 144, 129,\n        94,  55,   9, 127, 100,  78, 129,  76,  76,   9,  94, 161, 161,\n       161,  52,   9,  76,  56, 133,  56,   9, 100, 127, 129,  18, 158,\n       157, 167, 161,  52, 100, 157,   9, 164, 158,  76,   9, 133, 129,\n        18, 100,  11,  68, 164,  68,  96,   9,  18, 100,  99, 158,  75,\n        56,   9, 157,  55, 167, 100, 127,   9, 158,  11,  56,  76,  56,\n        55, 158,  75,  56, 129, 157, 157, 129,  77])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных\n",
        "\n"
      ],
      "metadata": {
        "id": "azltQy-gceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "OnahALhiceEl",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.759966Z",
          "iopub.execute_input": "2024-03-22T09:25:28.76028Z",
          "iopub.status.idle": "2024-03-22T09:25:28.769781Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.760247Z",
          "shell.execute_reply": "2024-03-22T09:25:28.768836Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Generates batches from encoded sequence.\n",
        "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
        "    :param batch_size: number of sequences per batch\n",
        "    :param seq_length: number of encoded chars in a sequence\n",
        "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
        "    \"\"\"\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "FLBOH8ZQUl7B",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.770935Z",
          "iopub.execute_input": "2024-03-22T09:25:28.771285Z",
          "iopub.status.idle": "2024-03-22T09:25:28.784146Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.771246Z",
          "shell.execute_reply": "2024-03-22T09:25:28.783165Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "metadata": {
        "id": "HlTnDntHceEl",
        "outputId": "c3e8ff5e-a271-4f02-a14d-3d3d0b3a2663",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.788186Z",
          "iopub.execute_input": "2024-03-22T09:25:28.788663Z",
          "iopub.status.idle": "2024-03-22T09:25:28.869583Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.788631Z",
          "shell.execute_reply": "2024-03-22T09:25:28.868538Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training on GPU!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VPq1EA38rBqn",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.874196Z",
          "iopub.execute_input": "2024-03-22T09:25:28.874654Z",
          "iopub.status.idle": "2024-03-22T09:25:28.890028Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.874615Z",
          "shell.execute_reply": "2024-03-22T09:25:28.888907Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель\n",
        "\n"
      ],
      "metadata": {
        "id": "5IrBRlEPceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    \"\"\"Training a network\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    net: CharRNN network\n",
        "    data: text data to train the network\n",
        "    epochs: Number of epochs to train\n",
        "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "    seq_length: Number of character steps per mini-batch\n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "metadata": {
        "id": "lv8VkRI0ceEl",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.89158Z",
          "iopub.execute_input": "2024-03-22T09:25:28.892164Z",
          "iopub.status.idle": "2024-03-22T09:25:28.912776Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.89213Z",
          "shell.execute_reply": "2024-03-22T09:25:28.911918Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определим модель\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
      ],
      "metadata": {
        "id": "Gt0q4KGEceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "ykMcIloEr3G7",
        "outputId": "eeaea24a-ba4f-4754-988a-47d44ffe1aa3",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:28.913798Z",
          "iopub.execute_input": "2024-03-22T09:25:28.914778Z",
          "iopub.status.idle": "2024-03-22T09:25:29.019302Z",
          "shell.execute_reply.started": "2024-03-22T09:25:28.914742Z",
          "shell.execute_reply": "2024-03-22T09:25:29.018372Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CharRNN(\n  (lstm): LSTM(172, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=172, bias=True)\n)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установим гиперпараметры"
      ],
      "metadata": {
        "id": "XHy6mECuceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 50\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=100,\n",
        ")"
      ],
      "metadata": {
        "id": "8hTkNrWEsjgI",
        "outputId": "ff95a494-f411-480c-e1ca-7068d97c8f17",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:25:29.020754Z",
          "iopub.execute_input": "2024-03-22T09:25:29.021498Z",
          "iopub.status.idle": "2024-03-22T09:46:58.23876Z",
          "shell.execute_reply.started": "2024-03-22T09:25:29.02147Z",
          "shell.execute_reply": "2024-03-22T09:46:58.237789Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch: 1/50... Step: 100... Loss: 3.3124... Val Loss: 3.4406\nEpoch: 1/50... Step: 200... Loss: 3.0320... Val Loss: 3.0231\nEpoch: 2/50... Step: 300... Loss: 2.7464... Val Loss: 2.7603\nEpoch: 2/50... Step: 400... Loss: 2.6462... Val Loss: 2.6370\nEpoch: 3/50... Step: 500... Loss: 2.5443... Val Loss: 2.5336\nEpoch: 3/50... Step: 600... Loss: 2.4779... Val Loss: 2.4507\nEpoch: 4/50... Step: 700... Loss: 2.4146... Val Loss: 2.3837\nEpoch: 4/50... Step: 800... Loss: 2.3818... Val Loss: 2.3287\nEpoch: 5/50... Step: 900... Loss: 2.3352... Val Loss: 2.2782\nEpoch: 5/50... Step: 1000... Loss: 2.2866... Val Loss: 2.2374\nEpoch: 5/50... Step: 1100... Loss: 2.2518... Val Loss: 2.1932\nEpoch: 6/50... Step: 1200... Loss: 2.2224... Val Loss: 2.1587\nEpoch: 6/50... Step: 1300... Loss: 2.2086... Val Loss: 2.1258\nEpoch: 7/50... Step: 1400... Loss: 2.1259... Val Loss: 2.0971\nEpoch: 7/50... Step: 1500... Loss: 2.1087... Val Loss: 2.0671\nEpoch: 8/50... Step: 1600... Loss: 2.1012... Val Loss: 2.0400\nEpoch: 8/50... Step: 1700... Loss: 2.0867... Val Loss: 2.0226\nEpoch: 9/50... Step: 1800... Loss: 2.0845... Val Loss: 1.9900\nEpoch: 9/50... Step: 1900... Loss: 2.0317... Val Loss: 1.9730\nEpoch: 9/50... Step: 2000... Loss: 2.0000... Val Loss: 1.9476\nEpoch: 10/50... Step: 2100... Loss: 2.0097... Val Loss: 1.9264\nEpoch: 10/50... Step: 2200... Loss: 1.9781... Val Loss: 1.9070\nEpoch: 11/50... Step: 2300... Loss: 1.9288... Val Loss: 1.8916\nEpoch: 11/50... Step: 2400... Loss: 1.9047... Val Loss: 1.8731\nEpoch: 12/50... Step: 2500... Loss: 1.9251... Val Loss: 1.8582\nEpoch: 12/50... Step: 2600... Loss: 1.8705... Val Loss: 1.8451\nEpoch: 13/50... Step: 2700... Loss: 1.8833... Val Loss: 1.8276\nEpoch: 13/50... Step: 2800... Loss: 1.8625... Val Loss: 1.8143\nEpoch: 13/50... Step: 2900... Loss: 1.8523... Val Loss: 1.8006\nEpoch: 14/50... Step: 3000... Loss: 1.8459... Val Loss: 1.7887\nEpoch: 14/50... Step: 3100... Loss: 1.8146... Val Loss: 1.7780\nEpoch: 15/50... Step: 3200... Loss: 1.8011... Val Loss: 1.7668\nEpoch: 15/50... Step: 3300... Loss: 1.8205... Val Loss: 1.7595\nEpoch: 16/50... Step: 3400... Loss: 1.8125... Val Loss: 1.7450\nEpoch: 16/50... Step: 3500... Loss: 1.7667... Val Loss: 1.7370\nEpoch: 17/50... Step: 3600... Loss: 1.7923... Val Loss: 1.7269\nEpoch: 17/50... Step: 3700... Loss: 1.7458... Val Loss: 1.7223\nEpoch: 17/50... Step: 3800... Loss: 1.7639... Val Loss: 1.7111\nEpoch: 18/50... Step: 3900... Loss: 1.7156... Val Loss: 1.7071\nEpoch: 18/50... Step: 4000... Loss: 1.7324... Val Loss: 1.6987\nEpoch: 19/50... Step: 4100... Loss: 1.7374... Val Loss: 1.6925\nEpoch: 19/50... Step: 4200... Loss: 1.7050... Val Loss: 1.6875\nEpoch: 20/50... Step: 4300... Loss: 1.7084... Val Loss: 1.6800\nEpoch: 20/50... Step: 4400... Loss: 1.7430... Val Loss: 1.6722\nEpoch: 21/50... Step: 4500... Loss: 1.6837... Val Loss: 1.6670\nEpoch: 21/50... Step: 4600... Loss: 1.6805... Val Loss: 1.6652\nEpoch: 21/50... Step: 4700... Loss: 1.7027... Val Loss: 1.6582\nEpoch: 22/50... Step: 4800... Loss: 1.6854... Val Loss: 1.6560\nEpoch: 22/50... Step: 4900... Loss: 1.7026... Val Loss: 1.6488\nEpoch: 23/50... Step: 5000... Loss: 1.6310... Val Loss: 1.6458\nEpoch: 23/50... Step: 5100... Loss: 1.6721... Val Loss: 1.6380\nEpoch: 24/50... Step: 5200... Loss: 1.6519... Val Loss: 1.6374\nEpoch: 24/50... Step: 5300... Loss: 1.6476... Val Loss: 1.6330\nEpoch: 25/50... Step: 5400... Loss: 1.6382... Val Loss: 1.6304\nEpoch: 25/50... Step: 5500... Loss: 1.6421... Val Loss: 1.6286\nEpoch: 25/50... Step: 5600... Loss: 1.6686... Val Loss: 1.6217\nEpoch: 26/50... Step: 5700... Loss: 1.6398... Val Loss: 1.6219\nEpoch: 26/50... Step: 5800... Loss: 1.6370... Val Loss: 1.6161\nEpoch: 27/50... Step: 5900... Loss: 1.6299... Val Loss: 1.6154\nEpoch: 27/50... Step: 6000... Loss: 1.6204... Val Loss: 1.6105\nEpoch: 28/50... Step: 6100... Loss: 1.6013... Val Loss: 1.6121\nEpoch: 28/50... Step: 6200... Loss: 1.5917... Val Loss: 1.6081\nEpoch: 29/50... Step: 6300... Loss: 1.6020... Val Loss: 1.6059\nEpoch: 29/50... Step: 6400... Loss: 1.6297... Val Loss: 1.6046\nEpoch: 30/50... Step: 6500... Loss: 1.6002... Val Loss: 1.6016\nEpoch: 30/50... Step: 6600... Loss: 1.6175... Val Loss: 1.6013\nEpoch: 30/50... Step: 6700... Loss: 1.5941... Val Loss: 1.5967\nEpoch: 31/50... Step: 6800... Loss: 1.5946... Val Loss: 1.5951\nEpoch: 31/50... Step: 6900... Loss: 1.5980... Val Loss: 1.5937\nEpoch: 32/50... Step: 7000... Loss: 1.5605... Val Loss: 1.5931\nEpoch: 32/50... Step: 7100... Loss: 1.5642... Val Loss: 1.5889\nEpoch: 33/50... Step: 7200... Loss: 1.5780... Val Loss: 1.5878\nEpoch: 33/50... Step: 7300... Loss: 1.5923... Val Loss: 1.5888\nEpoch: 34/50... Step: 7400... Loss: 1.5937... Val Loss: 1.5857\nEpoch: 34/50... Step: 7500... Loss: 1.5696... Val Loss: 1.5860\nEpoch: 34/50... Step: 7600... Loss: 1.5487... Val Loss: 1.5836\nEpoch: 35/50... Step: 7700... Loss: 1.5988... Val Loss: 1.5804\nEpoch: 35/50... Step: 7800... Loss: 1.5527... Val Loss: 1.5783\nEpoch: 36/50... Step: 7900... Loss: 1.5347... Val Loss: 1.5825\nEpoch: 36/50... Step: 8000... Loss: 1.5348... Val Loss: 1.5770\nEpoch: 37/50... Step: 8100... Loss: 1.5529... Val Loss: 1.5755\nEpoch: 37/50... Step: 8200... Loss: 1.5341... Val Loss: 1.5783\nEpoch: 38/50... Step: 8300... Loss: 1.5296... Val Loss: 1.5710\nEpoch: 38/50... Step: 8400... Loss: 1.5401... Val Loss: 1.5768\nEpoch: 38/50... Step: 8500... Loss: 1.5281... Val Loss: 1.5716\nEpoch: 39/50... Step: 8600... Loss: 1.5426... Val Loss: 1.5696\nEpoch: 39/50... Step: 8700... Loss: 1.5265... Val Loss: 1.5721\nEpoch: 40/50... Step: 8800... Loss: 1.5243... Val Loss: 1.5728\nEpoch: 40/50... Step: 8900... Loss: 1.5402... Val Loss: 1.5698\nEpoch: 41/50... Step: 9000... Loss: 1.5397... Val Loss: 1.5708\nEpoch: 41/50... Step: 9100... Loss: 1.5152... Val Loss: 1.5704\nEpoch: 42/50... Step: 9200... Loss: 1.5498... Val Loss: 1.5638\nEpoch: 42/50... Step: 9300... Loss: 1.4988... Val Loss: 1.5698\nEpoch: 42/50... Step: 9400... Loss: 1.5327... Val Loss: 1.5642\nEpoch: 43/50... Step: 9500... Loss: 1.4943... Val Loss: 1.5651\nEpoch: 43/50... Step: 9600... Loss: 1.5194... Val Loss: 1.5678\nEpoch: 44/50... Step: 9700... Loss: 1.5192... Val Loss: 1.5675\nEpoch: 44/50... Step: 9800... Loss: 1.5017... Val Loss: 1.5646\nEpoch: 45/50... Step: 9900... Loss: 1.5131... Val Loss: 1.5645\nEpoch: 45/50... Step: 10000... Loss: 1.5438... Val Loss: 1.5650\nEpoch: 46/50... Step: 10100... Loss: 1.4958... Val Loss: 1.5598\nEpoch: 46/50... Step: 10200... Loss: 1.5107... Val Loss: 1.5644\nEpoch: 46/50... Step: 10300... Loss: 1.5130... Val Loss: 1.5590\nEpoch: 47/50... Step: 10400... Loss: 1.5189... Val Loss: 1.5626\nEpoch: 47/50... Step: 10500... Loss: 1.5229... Val Loss: 1.5624\nEpoch: 48/50... Step: 10600... Loss: 1.4799... Val Loss: 1.5669\nEpoch: 48/50... Step: 10700... Loss: 1.5138... Val Loss: 1.5622\nEpoch: 49/50... Step: 10800... Loss: 1.4851... Val Loss: 1.5625\nEpoch: 49/50... Step: 10900... Loss: 1.5049... Val Loss: 1.5603\nEpoch: 50/50... Step: 11000... Loss: 1.4730... Val Loss: 1.5603\nEpoch: 50/50... Step: 11100... Loss: 1.4938... Val Loss: 1.5623\nEpoch: 50/50... Step: 11200... Loss: 1.5307... Val Loss: 1.5591\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfZxvNoDceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = \"char_lstm_50_epoch.net\"\n",
        "\n",
        "# checkpoint = {\n",
        "#     \"n_hidden\": net.n_hidden,\n",
        "#     \"n_layers\": net.n_layers,\n",
        "#     \"state_dict\": net.state_dict(),\n",
        "#     \"tokens\": net.chars,\n",
        "# }\n",
        "\n",
        "# with open(model_name, \"wb\") as f:\n",
        "#     torch.save(checkpoint, f)\n",
        "\n",
        "torch.save(net, model_name)"
      ],
      "metadata": {
        "id": "q6RXl5VAceEm",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:58.240088Z",
          "iopub.execute_input": "2024-03-22T09:46:58.24053Z",
          "iopub.status.idle": "2024-03-22T09:46:58.286952Z",
          "shell.execute_reply.started": "2024-03-22T09:46:58.240502Z",
          "shell.execute_reply": "2024-03-22T09:46:58.285906Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Делаем предсказания\n",
        "\n"
      ],
      "metadata": {
        "id": "K2sJhx5iceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "metadata": {
        "id": "QEIRW_B2ceEm",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:58.288489Z",
          "iopub.execute_input": "2024-03-22T09:46:58.289259Z",
          "iopub.status.idle": "2024-03-22T09:46:58.30866Z",
          "shell.execute_reply.started": "2024-03-22T09:46:58.289223Z",
          "shell.execute_reply": "2024-03-22T09:46:58.307893Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Priming и генерирование текста\n",
        "\n",
        "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
      ],
      "metadata": {
        "id": "OG38j3gQceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime=\"Инканденца\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "metadata": {
        "id": "P9vpB5gRceEm",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:58.309849Z",
          "iopub.execute_input": "2024-03-22T09:46:58.310176Z",
          "iopub.status.idle": "2024-03-22T09:46:58.321681Z",
          "shell.execute_reply.started": "2024-03-22T09:46:58.31015Z",
          "shell.execute_reply": "2024-03-22T09:46:58.320904Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime=\"Орин Инканденца\", top_k=2))"
      ],
      "metadata": {
        "id": "BqmFA9eEceEm",
        "outputId": "e37c8211-e4be-4444-ca45-887da70e9663",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:58.322765Z",
          "iopub.execute_input": "2024-03-22T09:46:58.323059Z",
          "iopub.status.idle": "2024-03-22T09:46:59.098431Z",
          "shell.execute_reply.started": "2024-03-22T09:46:58.323033Z",
          "shell.execute_reply": "2024-03-22T09:46:59.097397Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Орин Инканденца на серьезных получаем состоянии стоит всегда под сталом привыкне с подобной синей под просторной полоске, и по себе все время приходилось приставлять сверху по полу и под получает своим старым состоянием в своем привыке из стали приступающими стрессовой старой подобное получилось под столом, и призрачно принимает в подносите по столом, и в конце картриджа проблемы по себе под собой выпускал в какое-то время на предмет по полу в пространстве, из стола и стола, как под собой, когда поставит под столом в половину состояния в призраке, и потому что все только на степене словно все в столовой с половиной и поднятыми подобными половинам и странно стали, и продолжал в своей подозрительной странной силой и сказал, что она не поднялась, и под старом стальном старом состоянии просто не понял и не понимаешь, когда привели в получить в своей половине стола в пространстве и собственно страшно, как она приходит на полу просто при этом не словно в получить при себя на полу по себе в старом столовой п\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a checkpoint"
      ],
      "metadata": {
        "id": "942mjdQHceEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цитата из текста: 'Талант сам по себе ожидание... либо оправдываешь его, либо он машет платочком, исчезая навсегда.'"
      ],
      "metadata": {
        "id": "OhKGVUj2k2Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = torch.load(model_name)"
      ],
      "metadata": {
        "id": "c-qnXN8N6E6W",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:59.099997Z",
          "iopub.execute_input": "2024-03-22T09:46:59.100408Z",
          "iopub.status.idle": "2024-03-22T09:46:59.116644Z",
          "shell.execute_reply.started": "2024-03-22T09:46:59.10037Z",
          "shell.execute_reply": "2024-03-22T09:46:59.115645Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 1000, top_k=5, prime=\"Талант сам по себе ожидание\"))"
      ],
      "metadata": {
        "id": "Ut6R3zDcceEm",
        "outputId": "0bdd0ddd-6762-4fdd-fdba-067f3162ba62",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:59.11791Z",
          "iopub.execute_input": "2024-03-22T09:46:59.118234Z",
          "iopub.status.idle": "2024-03-22T09:46:59.831568Z",
          "shell.execute_reply.started": "2024-03-22T09:46:59.118194Z",
          "shell.execute_reply": "2024-03-22T09:46:59.830351Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Талант сам по себе ожидание. К тому, что он на коленях на парках на конце старика стерых сильно не пришло при предплечье. Последовало волнебное и то, что он не просил. Приветственный, с краю верхней дирактированным стеклям, что он поднимают в отрубаемую спортивную маска в такой период при этом первым, пока не понятия. Но не пахнет, как старый старший свет всегда скрывался, персила по воздействие, и просто все выпороды в сторону с круглыми коленками, которые великим страном. Выбулильным обломком.\n\n\n\n– Ты видим, как полная клиника с толпы и спасников и случайный постель в самоудистие, и на комнате просили. Я не представляет.\n\n– В общем, в общем, вы на примерно потому, что в конце пальцев, по пробум в столовой с себо. Все равно не выпустив папочку на высотых парковки, и потом не понял, – сказал он, которой правала принесла на свате выражения, и потом выбрал в крыси.\n\n– Ты сказал, что просто после выпукленных контролях, не после того, чтобы подозреваемо, не столько слушали со всем осознанием.\n\n– Янки и подобное.\n\n– Я\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-Level LSTM"
      ],
      "metadata": {
        "id": "pHtthF0BI1RO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токенизация"
      ],
      "metadata": {
        "id": "vWgeBZ8sJCuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eNs1iVMZC3s8",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:59.833063Z",
          "iopub.execute_input": "2024-03-22T09:46:59.833459Z",
          "iopub.status.idle": "2024-03-22T09:46:59.839581Z",
          "shell.execute_reply.started": "2024-03-22T09:46:59.833426Z",
          "shell.execute_reply": "2024-03-22T09:46:59.838371Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "id": "ZiIvJrBcC8ne",
        "outputId": "9b9a9a27-6b7a-4d3d-dc2e-8d04e6b4f29e",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:59.84102Z",
          "iopub.execute_input": "2024-03-22T09:46:59.841448Z",
          "iopub.status.idle": "2024-03-22T09:46:59.855839Z",
          "shell.execute_reply.started": "2024-03-22T09:46:59.841402Z",
          "shell.execute_reply": "2024-03-22T09:46:59.854831Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Бесконечная шутка\\nДэвид Фостер Уоллес\\n\\n\\nВеликие романы\\nВ недалеком будущем пациенты реабилитационной'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизируем слова\n",
        "# Разделяем текст на\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "vocab = list(word_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "SEQUENCE_LENGTH = 64\n",
        "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n",
        "print(len(vocab))\n",
        "print(len(word_to_int))\n",
        "print(int_to_word[2])"
      ],
      "metadata": {
        "id": "82nMjOO1JEtK",
        "outputId": "decfa876-59ec-4dcf-9021-48c5d5f3779a",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:46:59.857069Z",
          "iopub.execute_input": "2024-03-22T09:46:59.857393Z",
          "iopub.status.idle": "2024-03-22T09:47:01.723526Z",
          "shell.execute_reply.started": "2024-03-22T09:46:59.857365Z",
          "shell.execute_reply": "2024-03-22T09:47:01.722383Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "117706\n117706\nДэвид\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders"
      ],
      "metadata": {
        "id": "cWlHPlVoJMOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, samples, word_to_int):\n",
        "        self.samples = samples\n",
        "        self.word_to_int = word_to_int\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
        "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
        "        return input_seq, target_seq"
      ],
      "metadata": {
        "id": "XApGjnBDJOtb",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:47:01.724985Z",
          "iopub.execute_input": "2024-03-22T09:47:01.725371Z",
          "iopub.status.idle": "2024-03-22T09:47:01.732413Z",
          "shell.execute_reply.started": "2024-03-22T09:47:01.725338Z",
          "shell.execute_reply": "2024-03-22T09:47:01.731479Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = TextDataset(samples, word_to_int)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "print(dataset[1])"
      ],
      "metadata": {
        "id": "MdF1B42qJUs1",
        "outputId": "2fc63c84-f1b9-4017-fcc9-fc03a3e3551d",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:47:01.733774Z",
          "iopub.execute_input": "2024-03-22T09:47:01.734036Z",
          "iopub.status.idle": "2024-03-22T09:47:01.746958Z",
          "shell.execute_reply.started": "2024-03-22T09:47:01.734014Z",
          "shell.execute_reply": "2024-03-22T09:47:01.746087Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n        19, 20, 21, 22, 14, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n        54, 55, 14, 56, 57, 58, 59, 60, 61, 62]), tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n        20, 21, 22, 14, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n        37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n        55, 14, 56, 57, 58, 59, 60, 61, 62, 63]))\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerationLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_size,\n",
        "        num_layers\n",
        "    ):\n",
        "        super(TextGenerationLSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # nn.init.xavier_uniform_(self.lstm.weight)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        if hidden == None:\n",
        "            hidden = self.init_hidden(x.shape[0])\n",
        "        x = self.embedding(x)\n",
        "        out, (h_n, c_n) = self.lstm(x, hidden)\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, (h_n, c_n)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return h0, c0"
      ],
      "metadata": {
        "id": "gyVHSZrpJYWa",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:47:01.748364Z",
          "iopub.execute_input": "2024-03-22T09:47:01.748674Z",
          "iopub.status.idle": "2024-03-22T09:47:01.758986Z",
          "shell.execute_reply.started": "2024-03-22T09:47:01.748644Z",
          "shell.execute_reply": "2024-03-22T09:47:01.758032Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установим гиперпараметры"
      ],
      "metadata": {
        "id": "9GFn2rz0Jt3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Setup\n",
        "embedding_dim = 32\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "learning_rate = 0.01\n",
        "epochs = 6"
      ],
      "metadata": {
        "id": "y-_sh96BJe2e",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:47:01.75997Z",
          "iopub.execute_input": "2024-03-22T09:47:01.760231Z",
          "iopub.status.idle": "2024-03-22T09:47:01.775945Z",
          "shell.execute_reply.started": "2024-03-22T09:47:01.760208Z",
          "shell.execute_reply": "2024-03-22T09:47:01.775193Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextGenerationLSTM(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    hidden_size,\n",
        "    num_layers\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "uFO7uIuBJg2F",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:47:01.777067Z",
          "iopub.execute_input": "2024-03-22T09:47:01.777389Z",
          "iopub.status.idle": "2024-03-22T09:47:01.918232Z",
          "shell.execute_reply.started": "2024-03-22T09:47:01.777315Z",
          "shell.execute_reply": "2024-03-22T09:47:01.917353Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель"
      ],
      "metadata": {
        "id": "7l7xh4WUJpTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(model, epochs, dataloader, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "            outputs, _ = model(input_seq)\n",
        "            loss = criterion(outputs, target_seq.view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.detach().cpu().numpy()\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")\n",
        "train(model, epochs, dataloader, criterion)"
      ],
      "metadata": {
        "id": "dZqBEnAtJixz",
        "outputId": "a9ceec22-7372-47a8-da65-05c04569299d",
        "execution": {
          "iopub.status.busy": "2024-03-22T09:47:01.923287Z",
          "iopub.execute_input": "2024-03-22T09:47:01.923618Z",
          "iopub.status.idle": "2024-03-22T11:45:22.7648Z",
          "shell.execute_reply.started": "2024-03-22T09:47:01.923591Z",
          "shell.execute_reply": "2024-03-22T11:45:22.763653Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 0 loss: 2.465\nEpoch 1 loss: 1.355\nEpoch 2 loss: 1.224\nEpoch 3 loss: 1.171\nEpoch 4 loss: 1.149\nEpoch 5 loss: 1.144\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "EnddYnexJoft",
        "outputId": "bafa3d0b-4650-4a56-8771-c7db1af1d81d",
        "execution": {
          "iopub.status.busy": "2024-03-22T11:45:24.069418Z",
          "iopub.execute_input": "2024-03-22T11:45:24.069743Z",
          "iopub.status.idle": "2024-03-22T11:45:24.082261Z",
          "shell.execute_reply.started": "2024-03-22T11:45:24.069713Z",
          "shell.execute_reply": "2024-03-22T11:45:24.081156Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "TextGenerationLSTM(\n  (embedding): Embedding(117706, 32)\n  (lstm): LSTM(32, 64, batch_first=True)\n  (fc): Linear(in_features=64, out_features=117706, bias=True)\n)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Делаем предсказания"
      ],
      "metadata": {
        "id": "-BGOynuHJ7k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = '/kaggle/working/word_lstm_5_epoch.net'\n",
        "torch.save(model, model_name)"
      ],
      "metadata": {
        "id": "0li8wL2dKGFv",
        "execution": {
          "iopub.status.busy": "2024-03-22T11:45:22.773427Z",
          "iopub.execute_input": "2024-03-22T11:45:22.773744Z",
          "iopub.status.idle": "2024-03-22T11:45:22.903553Z",
          "shell.execute_reply.started": "2024-03-22T11:45:22.773717Z",
          "shell.execute_reply": "2024-03-22T11:45:22.902378Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-22T11:45:22.904974Z",
          "iopub.execute_input": "2024-03-22T11:45:22.905357Z",
          "iopub.status.idle": "2024-03-22T11:45:22.911845Z",
          "shell.execute_reply.started": "2024-03-22T11:45:22.9053Z",
          "shell.execute_reply": "2024-03-22T11:45:22.910726Z"
        },
        "trusted": true,
        "id": "pgVRC8blBChw",
        "outputId": "9e94d240-9b53-4815-e5fd-c6cb5d780219"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'/kaggle/working/word_lstm_5_epoch.net'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_word = torch.load(model_name)"
      ],
      "metadata": {
        "id": "FPsOIDj7KiZj",
        "execution": {
          "iopub.status.busy": "2024-03-22T11:45:22.913057Z",
          "iopub.execute_input": "2024-03-22T11:45:22.91341Z",
          "iopub.status.idle": "2024-03-22T11:45:22.947288Z",
          "shell.execute_reply.started": "2024-03-22T11:45:22.913378Z",
          "shell.execute_reply": "2024-03-22T11:45:22.94637Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_words):\n",
        "    model.eval()\n",
        "    words = start_string.split()\n",
        "    for _ in range(num_words):\n",
        "        input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0).to(device)\n",
        "        h, c = model.init_hidden(1)\n",
        "        output, (h, c) = model(input_seq, (h, c))\n",
        "        next_token = output.argmax(1)[-1].item()\n",
        "        words.append(int_to_word[next_token])\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Example usage:\n",
        "print(\"Generated Text:\", generate_text(loaded_word, start_string=\"Талант сам по себе\", num_words=1000))"
      ],
      "metadata": {
        "id": "53dljl7hJ6-g",
        "outputId": "93faccb0-3ccc-4414-ffcc-9f079eb761a5",
        "execution": {
          "iopub.status.busy": "2024-03-22T11:45:22.948601Z",
          "iopub.execute_input": "2024-03-22T11:45:22.948915Z",
          "iopub.status.idle": "2024-03-22T11:45:24.056572Z",
          "shell.execute_reply.started": "2024-03-22T11:45:22.948887Z",
          "shell.execute_reply": "2024-03-22T11:45:24.05561Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated Text: Талант сам по себе метаболезнь.) 239. Из-за клятвы хранить тайну Хэл высоко над столом, как гипертоническая луна, ни разу не отвернулось. С блестящей росой соплей в усах. И даже не спрашивай о нем как не менее чем дюжиной элитных конных кирасиров МВД, тем не менее услышали скрип прошлой ночью у сетки с мальчиком и Западного девушка точно играл из больного зуба. Когда все приступают к разминке, выстроившись рядами вдоль задних линий и линий подачи, делая повороты и наклоны, преклоняя колени перед пустотой, меняя позы у нее есть Дом ректора и кабинет, и туннель между ними, и с ней падает ничком и навсегда бросил на ходу. Развалюсь в Легком, или в Тусоне на жаре в 200 градусов в течение двухнедельного интервала «отрави-тунцом-и-беги» для Юэлла и наклоняясь в черных женских деловых костюмах от «Шанель» с дипломами Уэлсли и планшетами; и но, наконец, некоторые раз, пока Орто Стайс бросит возиться с носками и рассеянно и второго Хэла которых возле из спальни над которой она нависает над ведром для переутомлений, уставившись в ожидании П. в журналах. Находят в них жизненный смысл. – Мне кажется. Они находят. И я буду. Иначе почему он героически заступался в кино Гейтли, Шахт не употребляет обороты «всенепременно», «зримое» или «действующая лимбическая система», хотя чуть ранее для как. и тренировок со на площади Инмана и Фортье снова и снова, на недели, Он в Чайнатауне в копеечку влетит и все такое, – говорит кухонное Несколько трескать кондиционера к стенке На пару месяцев и уйдет. коронарного стента[48] (#local_48). Он подумал: «Умирать дважды». Марат сказал: – Если она узнает, что мы соразмерно с кем-либо поладить. Или пока. который может все, кроме него. Но вроде Грин, что вслух. От другого Бун, становится на мнению пальца и ждут, доверившись вибрации железнодорожных шпал и камертону свистка, играли у него в программу, вижу семь, сказал, что даже кривишься! Гостиная забитая и душная, играет пошлое мамбо, стены все еще распространено, и назад мяч на карте и применила кофеварки передач и настоящую причину при некое тех пор т. к. Аврил постукивает по зубам синим фломастером. «Бабушка щиплет меня за щеку», – подает голос вскоре огромная голова. Совершенно несоразмерная, когда Гейтли определить жар. Определенно риновирус, внезапный и свирепый. Он размышлял: когда вчера Грэм Рэйдер притворился, что чихнул на поднос с обедом Д. Трельча и позади волос. Плотность надел змей человеком, способным испытывать Абсолютное Беспокойство, но не впасть в паралич от беспокойства или отсутствия копчиком и какие-то военный 1 на 400 000 кюри стронция-90, – т. е. была какая-то проблема и все такое, и тачилу отогнать к одному понимающему пиздоглазому из Чайнатауна, с которым он корешился, но Бедный Тони и ваще по сравнению с которым можно поговорить. Для – Если не можешь выбрать – это мои друзья также: лишены ног. – Видимо, крушение поезда с новеньким и ослепительно-белым, как бескозырка матроса, гипсом без единой подписи, за ним – бесстрастный проректор Тони и потопали в библатеку на Копли, где чувствуя себя обновленными. Ты здесь. Настало время беседы. Может быть, обсудим византийскую эротику? – Как вы узнали, что мы персонаж Берне организовал что-нибудь, с чем мы к другу. Самого. Тридцать шесть Западного в Соединенных Штатов Родни школы стараясь восстановить ту утраченную близость с Мадам Психоз, для чего не содержит вспышек света, верно? МИСТЕР ВИЛС: Ну но Гейтли глотает «р» из-за акцента Северного побережья или из-за детской картавости. 212 Их райской страны (фр.). 213 Глина (фр.). 214 Федеральное управление гражданской авиации США. 215 «Райе Криспи» – воздушный рис. 216 Все персонажи, включая Одноглазого Пони, – реальные. 217 Слово-призрак – калька с англоязычного понятия ghostword, слово, которое не было. Мать гнев, – лишь бы на личную химию, чтобы сладить с особыми требованиями ЭТА: декседрин или низковольтный метедрин[5] (#local_5) перед матчами и бензодиазепины[6] (#local_6), чтобы сняться после, с «Оползнями» или «Синим пламенем» в каком-нибудь понимающем ночном клубе[7] (#local_7) на Содружке или с пивом и бонгами в отдаленном уголке академии по ночам, замыкая цикл драйва и расслабона, с грибами или X, или чем-нибудь из класса «легких дизайнерских» [8] (#local_8) – а может, иногда я от игрока на защите и что при наследственной вегетативно-сосудистой дистонии – нечувствительны пленки за всем как будто разрывалось. – Не зная – Но крики Лайл отмахивается от обид, решение Стипли. – У меня и требуется. Я сижу Марио. банку, кабельная камарилья во главе с Малоуном, Тернером и таинственным альбертанцем переманила в ИСКРу даже один раз, в чем не важно, чтобы она видела, как я поступала раньше: выбрасывала трубку, грозила небу кулаком и заявляла: «Бог мне гребанный свидетель – БОЛЬШЕ НИКОГДА, с этой минуты, прямо под вуалью. – Вернерсберг Пенсильвании. – Ну «Чит-Чат» мы знаем, что в отличие от степени безрассудно, потенциально, как Билла-Восьмидесятника, безукоризненно ухоженного чувака, носившего красные подтяжки под козырными мужскими костюмами Zegna, очки в черепаховой оправе и «Доксайдеры», старомодного корпоративного поглотителя и капиталорасхитителя, с виду лет пятидесяти, с офисом на станции «Эксчейндж Плейс» и сувенирным стикером «Свободу Милкену»[229 - Майкл Милкен (1946) – основатель рынка мусорных облигаций и организатор захватов гигантских лжи. – Эй, Хэл? – С его картой обычно лучи – Да я не разговариваю с кем нельзя было назвать красивым из каприза чрезвычайно насолить на утренних занятиях. Одна из причин, почему я перезвонил. вслед инженер Пемулису. – солнце еще в Уэстоне, определенные тики и заминки в подаче или приеме мяча на лету, чтобы преподаватели показывали записи во время инструктажей, наглядно демонстрируя ребятам, о чем конкретно у всех детей позади из спальни родителей и положив руку, а то я о штатам, но не было придумать Стайс пришел, к огромным дела, и верят, что он сидит по собственной воле. Бедный Тони Краузе в ЭТА, когда он не хотел, чтобы испытывал искушение в самом строгом смысле ушел за спиной и вся, всем в ладони, без бессознательного – в роли тенниса, протягивает экран и он помог Джоэль Брюс кусок плесени. доктор By к нему, до сих пор, как правило, прибывают на следующей цветовой схемой: коричневый, лавандовый и либо мятно-зеленый, либо бледно-желтый. – Я перезвоню, когда ты придешь в себя. Все расслышал как срочно что-то делал на боку, перед жрачкой и хлестнула вверх, по желтому мячу в паре кварталов севернее на Бишоп-Аллен-Драйв, где лоб пульсирует. В общем, разговор\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы:**\n",
        "\n",
        "- Построены две модели разных подходов: посимвольной генерации и пословной.\n",
        "- Метрики моделей разнятся. Результат лучше у подхода пословной генерации\n",
        "- Конечный результат далёк от идеального. Возможно это из-за выбранного текста: объёмный, большое количество слов придумано автором, в данном случае, мы считываем перевод английского изначально достаточно сложного текста, который принципиально непереводим. Так что возможно это повлияло на результат,либо возможно следовало усложнить модели."
      ],
      "metadata": {
        "id": "tePY0txNfBe7"
      }
    }
  ]
}