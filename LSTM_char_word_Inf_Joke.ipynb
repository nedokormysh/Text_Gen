{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "LSTM_char_word_Inf_Joke",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7594846,
          "sourceType": "datasetVersion",
          "datasetId": 4420678
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Text_Gen_RNN/blob/model/LSTM_char_word_Inf_Joke.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'infinite-joke-txt:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4420678%2F7594846%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240321%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240321T135614Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db2b8f856607e2399c82ca76a49b42f74b0e353d9b9a92346b9244c90c01a637b58687e43b206b755be2d3174a649bc02b6f2bbc682f1cebfe83267cafb165f4abdc75c0ed6ff004ab3625efb12c98ff1c2bccf1a704aa982db294f4f8b0333028eaabfe14726a22e970ff28d128b8e12f4d188be2e7cc4623176c8bf9aaec65d8684a1e94a273cd364b2b83d23d4da1d300b24125ff866d0d0249d2311b34b7dcfeabb0670e466ae3f07ecda3ddb395162db4392c6bcb03c3d3a50678a9a437dfd9d760504d01b32f7134b4e762086b455292fe659e1031b3ae3aa3527666c1854634eb8f09b33bbce82bde444c7c65f2f3e83a97ec7a96d9a81c4c0e6c24e7e'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "N26dhryqTF98",
        "outputId": "19636435-9d64-4e9a-f5aa-64aa52125420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading infinite-joke-txt, 1847729 bytes compressed\n",
            "[==================================================] 1847729 bytes downloaded\n",
            "Downloaded and uncompressed: infinite-joke-txt\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/Stepik_Ai_edu_RNN/blob/week_5_char_RNN/AiEdu_CharRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character-Level LSTM\n",
        "Обучим модель на тексте книги \"Бесконечная шутка\", после чего попробуем генерировать новый текст."
      ],
      "metadata": {
        "id": "1W8R8WgZceEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "sqUOE2flceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:33.119187Z",
          "iopub.execute_input": "2024-03-21T11:31:33.119977Z",
          "iopub.status.idle": "2024-03-21T11:31:39.034218Z",
          "shell.execute_reply.started": "2024-03-21T11:31:33.119938Z",
          "shell.execute_reply": "2024-03-21T11:31:39.033438Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# фиксируем воспроизводимость\n",
        "def seed_everything(seed=42):\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed=7575)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:39.036015Z",
          "iopub.execute_input": "2024-03-21T11:31:39.036795Z",
          "iopub.status.idle": "2024-03-21T11:31:39.04655Z",
          "shell.execute_reply.started": "2024-03-21T11:31:39.036756Z",
          "shell.execute_reply": "2024-03-21T11:31:39.045778Z"
        },
        "trusted": true,
        "id": "sTlL2swVTF-B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузим данные\n"
      ],
      "metadata": {
        "id": "_wHfCDyzceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # open text file and read in data as `text`\n",
        "# with open(\"/content/Uolles_D._Velikieromanyi._Beskonechnaya_Shutka.txt\", \"r\") as f:\n",
        "#     text = f.read()\n",
        "\n",
        "with open(\"/kaggle/input/infinite-joke-txt/Uolles_D._Velikieromanyi._Beskonechnaya_Shutka.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "b34kfqIOceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:39.047769Z",
          "iopub.execute_input": "2024-03-21T11:31:39.04814Z",
          "iopub.status.idle": "2024-03-21T11:31:39.245137Z",
          "shell.execute_reply.started": "2024-03-21T11:31:39.048107Z",
          "shell.execute_reply": "2024-03-21T11:31:39.244406Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим первые несколько символов:"
      ],
      "metadata": {
        "id": "Jp1Ljc4mceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[:108]"
      ],
      "metadata": {
        "id": "7VctmLQfceEl",
        "outputId": "7a1cab31-6510-408a-d729-ef68b82bbc90",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:39.24715Z",
          "iopub.execute_input": "2024-03-21T11:31:39.247482Z",
          "iopub.status.idle": "2024-03-21T11:31:39.254006Z",
          "shell.execute_reply.started": "2024-03-21T11:31:39.247456Z",
          "shell.execute_reply": "2024-03-21T11:31:39.253077Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Бесконечная шутка\\nДэвид Фостер Уоллес\\n\\n\\nВеликие романы\\nВ недалеком будущем пациенты реабилитационной клиники'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация\n",
        "\n",
        "В ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно. Кодирование символов как целых чисел упрощает их использование в качестве входных данных в сети."
      ],
      "metadata": {
        "id": "4iC21bopceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "tYVlmnxLceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:39.254906Z",
          "iopub.execute_input": "2024-03-21T11:31:39.255134Z",
          "iopub.status.idle": "2024-03-21T11:31:40.090032Z",
          "shell.execute_reply.started": "2024-03-21T11:31:39.255114Z",
          "shell.execute_reply": "2024-03-21T11:31:40.08904Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим как символы закодировались целыми числами"
      ],
      "metadata": {
        "id": "oJIzwzSwceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded[:100]"
      ],
      "metadata": {
        "id": "WK1MYr_9ceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.091232Z",
          "iopub.execute_input": "2024-03-21T11:31:40.091562Z",
          "iopub.status.idle": "2024-03-21T11:31:40.09832Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.091535Z",
          "shell.execute_reply": "2024-03-21T11:31:40.097433Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных\n",
        "\n"
      ],
      "metadata": {
        "id": "azltQy-gceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.0\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "OnahALhiceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.099934Z",
          "iopub.execute_input": "2024-03-21T11:31:40.100259Z",
          "iopub.status.idle": "2024-03-21T11:31:40.110034Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.100231Z",
          "shell.execute_reply": "2024-03-21T11:31:40.10907Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(int_words, batch_size, seq_length):\n",
        "    \"\"\"\n",
        "    Generates batches from encoded sequence.\n",
        "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
        "    :param batch_size: number of sequences per batch\n",
        "    :param seq_length: number of encoded chars in a sequence\n",
        "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
        "    \"\"\"\n",
        "    # 1. Truncate text, so there are only full batches\n",
        "    window_size = seq_length + 1\n",
        "    batch_size_total = batch_size * window_size\n",
        "    n_batches = len(int_words) // batch_size_total\n",
        "    int_words = int_words[: n_batches * batch_size_total]\n",
        "\n",
        "    # 2. Reshape into batch_size rows\n",
        "    int_words = int_words.reshape((batch_size, -1))\n",
        "\n",
        "    # 3. Iterate through the text matrix\n",
        "    for position in range(0, int_words.shape[1], window_size):\n",
        "        x = int_words[:, position : position + window_size - 1]\n",
        "        y = int_words[:, position + 1 : position + window_size]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "FLBOH8ZQUl7B",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.111506Z",
          "iopub.execute_input": "2024-03-21T11:31:40.111847Z",
          "iopub.status.idle": "2024-03-21T11:31:40.122264Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.111821Z",
          "shell.execute_reply": "2024-03-21T11:31:40.121451Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if train_on_gpu:\n",
        "    print(\"Training on GPU!\")\n",
        "else:\n",
        "    print(\"No GPU available, training on CPU; consider making n_epochs very small.\")"
      ],
      "metadata": {
        "id": "HlTnDntHceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.123528Z",
          "iopub.execute_input": "2024-03-21T11:31:40.123832Z",
          "iopub.status.idle": "2024-03-21T11:31:40.205313Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.123802Z",
          "shell.execute_reply": "2024-03-21T11:31:40.204319Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"Forward pass through the network.\n",
        "        These inputs are x, and the hidden/cell state `hidden`.\"\"\"\n",
        "\n",
        "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "\n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "\n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initializes hidden state\"\"\"\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if train_on_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "VPq1EA38rBqn",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.209433Z",
          "iopub.execute_input": "2024-03-21T11:31:40.209703Z",
          "iopub.status.idle": "2024-03-21T11:31:40.222352Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.20968Z",
          "shell.execute_reply": "2024-03-21T11:31:40.22136Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель\n",
        "\n"
      ],
      "metadata": {
        "id": "5IrBRlEPceEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    net,\n",
        "    data,\n",
        "    epochs=10,\n",
        "    batch_size=10,\n",
        "    seq_length=50,\n",
        "    lr=0.001,\n",
        "    clip=5,\n",
        "    val_frac=0.1,\n",
        "    print_every=10,\n",
        "):\n",
        "    \"\"\"Training a network\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    net: CharRNN network\n",
        "    data: text data to train the network\n",
        "    epochs: Number of epochs to train\n",
        "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "    seq_length: Number of character steps per mini-batch\n",
        "    lr: learning rate\n",
        "    clip: gradient clipping\n",
        "    val_frac: Fraction of data to hold out for validation\n",
        "    print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if train_on_gpu:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if train_on_gpu:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output, targets.view(batch_size * seq_length).long()\n",
        "                    )\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train()  # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\n",
        "                    \"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
        "                )"
      ],
      "metadata": {
        "id": "lv8VkRI0ceEl",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.224803Z",
          "iopub.execute_input": "2024-03-21T11:31:40.225102Z",
          "iopub.status.idle": "2024-03-21T11:31:40.244537Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.225079Z",
          "shell.execute_reply": "2024-03-21T11:31:40.243381Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определим модель\n",
        "\n",
        "Теперь мы можем создать модель с заданными гиперпараметрами. Определим размеры мини-батчей."
      ],
      "metadata": {
        "id": "Gt0q4KGEceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "metadata": {
        "id": "ykMcIloEr3G7",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.245744Z",
          "iopub.execute_input": "2024-03-21T11:31:40.246546Z",
          "iopub.status.idle": "2024-03-21T11:31:40.352967Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.246522Z",
          "shell.execute_reply": "2024-03-21T11:31:40.351892Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Установим гиперпараметры"
      ],
      "metadata": {
        "id": "XHy6mECuceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 50\n",
        "\n",
        "# train the model\n",
        "train(\n",
        "    net,\n",
        "    encoded,\n",
        "    epochs=n_epochs,\n",
        "    batch_size=batch_size,\n",
        "    seq_length=seq_length,\n",
        "    lr=0.001,\n",
        "    print_every=100,\n",
        ")"
      ],
      "metadata": {
        "id": "8hTkNrWEsjgI",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:31:40.354579Z",
          "iopub.execute_input": "2024-03-21T11:31:40.355281Z",
          "iopub.status.idle": "2024-03-21T11:51:12.366432Z",
          "shell.execute_reply.started": "2024-03-21T11:31:40.355241Z",
          "shell.execute_reply": "2024-03-21T11:51:12.365318Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoint\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfZxvNoDceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = \"char_lstm_50_epoch.net\"\n",
        "\n",
        "# checkpoint = {\n",
        "#     \"n_hidden\": net.n_hidden,\n",
        "#     \"n_layers\": net.n_layers,\n",
        "#     \"state_dict\": net.state_dict(),\n",
        "#     \"tokens\": net.chars,\n",
        "# }\n",
        "\n",
        "# with open(model_name, \"wb\") as f:\n",
        "#     torch.save(checkpoint, f)\n",
        "\n",
        "torch.save(net, model_name)"
      ],
      "metadata": {
        "id": "q6RXl5VAceEm",
        "execution": {
          "iopub.status.busy": "2024-03-21T13:52:31.723171Z",
          "iopub.execute_input": "2024-03-21T13:52:31.723565Z",
          "iopub.status.idle": "2024-03-21T13:52:31.750704Z",
          "shell.execute_reply.started": "2024-03-21T13:52:31.723533Z",
          "shell.execute_reply": "2024-03-21T13:52:31.749703Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Делаем предсказания\n",
        "\n"
      ],
      "metadata": {
        "id": "K2sJhx5iceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "    \"\"\"Given a character, predict the next character.\n",
        "    Returns the predicted character and the hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[net.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(net.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if train_on_gpu:\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    # get the output of the model\n",
        "    out, h = net(inputs, h)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if train_on_gpu:\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(net.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return net.int2char[char], h"
      ],
      "metadata": {
        "id": "QEIRW_B2ceEm",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:12.412614Z",
          "iopub.execute_input": "2024-03-21T11:51:12.412896Z",
          "iopub.status.idle": "2024-03-21T11:51:12.447219Z",
          "shell.execute_reply.started": "2024-03-21T11:51:12.412872Z",
          "shell.execute_reply": "2024-03-21T11:51:12.446507Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Priming и генерирование текста\n",
        "\n",
        "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
      ],
      "metadata": {
        "id": "OG38j3gQceEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime=\"Инканденца\", top_k=None):\n",
        "\n",
        "    if train_on_gpu:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return \"\".join(chars)"
      ],
      "metadata": {
        "id": "P9vpB5gRceEm",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:12.448244Z",
          "iopub.execute_input": "2024-03-21T11:51:12.448525Z",
          "iopub.status.idle": "2024-03-21T11:51:12.470452Z",
          "shell.execute_reply.started": "2024-03-21T11:51:12.448501Z",
          "shell.execute_reply": "2024-03-21T11:51:12.469415Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(net, 1000, prime=\"Орин Инканденца\", top_k=2))"
      ],
      "metadata": {
        "id": "BqmFA9eEceEm",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:12.471607Z",
          "iopub.execute_input": "2024-03-21T11:51:12.472704Z",
          "iopub.status.idle": "2024-03-21T11:51:13.22437Z",
          "shell.execute_reply.started": "2024-03-21T11:51:12.472668Z",
          "shell.execute_reply": "2024-03-21T11:51:13.223376Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a checkpoint"
      ],
      "metadata": {
        "id": "942mjdQHceEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цитата из текста: 'Талант сам по себе ожидание... либо оправдываешь его, либо он машет платочком, исчезая навсегда.'"
      ],
      "metadata": {
        "id": "OhKGVUj2k2Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = torch.load(model_name)"
      ],
      "metadata": {
        "id": "c-qnXN8N6E6W",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:13.22547Z",
          "iopub.execute_input": "2024-03-21T11:51:13.225786Z",
          "iopub.status.idle": "2024-03-21T11:51:13.241703Z",
          "shell.execute_reply.started": "2024-03-21T11:51:13.225752Z",
          "shell.execute_reply": "2024-03-21T11:51:13.240779Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 1000, top_k=5, prime=\"Талант сам по себе ожидание\"))"
      ],
      "metadata": {
        "id": "Ut6R3zDcceEm",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:13.243144Z",
          "iopub.execute_input": "2024-03-21T11:51:13.243939Z",
          "iopub.status.idle": "2024-03-21T11:51:13.883901Z",
          "shell.execute_reply.started": "2024-03-21T11:51:13.243905Z",
          "shell.execute_reply": "2024-03-21T11:51:13.882957Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-Level LSTM"
      ],
      "metadata": {
        "id": "pHtthF0BI1RO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токенизация"
      ],
      "metadata": {
        "id": "vWgeBZ8sJCuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eNs1iVMZC3s8",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:13.885098Z",
          "iopub.execute_input": "2024-03-21T11:51:13.885347Z",
          "iopub.status.idle": "2024-03-21T11:51:13.891232Z",
          "shell.execute_reply.started": "2024-03-21T11:51:13.885325Z",
          "shell.execute_reply": "2024-03-21T11:51:13.890349Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "id": "ZiIvJrBcC8ne",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:13.892344Z",
          "iopub.execute_input": "2024-03-21T11:51:13.892719Z",
          "iopub.status.idle": "2024-03-21T11:51:13.906161Z",
          "shell.execute_reply.started": "2024-03-21T11:51:13.892676Z",
          "shell.execute_reply": "2024-03-21T11:51:13.905279Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизируем слова\n",
        "# Разделяем текст на\n",
        "words = text.split()\n",
        "word_counts = Counter(words)\n",
        "vocab = list(word_counts.keys())\n",
        "vocab_size = len(vocab)\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "SEQUENCE_LENGTH = 64\n",
        "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n",
        "print(len(vocab))\n",
        "print(len(word_to_int))\n",
        "print(int_to_word[2])"
      ],
      "metadata": {
        "id": "82nMjOO1JEtK",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:13.907239Z",
          "iopub.execute_input": "2024-03-21T11:51:13.907541Z",
          "iopub.status.idle": "2024-03-21T11:51:15.62891Z",
          "shell.execute_reply.started": "2024-03-21T11:51:13.907517Z",
          "shell.execute_reply": "2024-03-21T11:51:15.627747Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders"
      ],
      "metadata": {
        "id": "cWlHPlVoJMOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, samples, word_to_int):\n",
        "        self.samples = samples\n",
        "        self.word_to_int = word_to_int\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        input_seq = torch.LongTensor([self.word_to_int[word] for word in sample[:-1]])\n",
        "        target_seq = torch.LongTensor([self.word_to_int[word] for word in sample[1:]])\n",
        "        return input_seq, target_seq"
      ],
      "metadata": {
        "id": "XApGjnBDJOtb",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:15.630074Z",
          "iopub.execute_input": "2024-03-21T11:51:15.630348Z",
          "iopub.status.idle": "2024-03-21T11:51:15.636973Z",
          "shell.execute_reply.started": "2024-03-21T11:51:15.630324Z",
          "shell.execute_reply": "2024-03-21T11:51:15.636051Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = TextDataset(samples, word_to_int)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "print(dataset[1])"
      ],
      "metadata": {
        "id": "MdF1B42qJUs1",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:15.638421Z",
          "iopub.execute_input": "2024-03-21T11:51:15.638791Z",
          "iopub.status.idle": "2024-03-21T11:51:15.65494Z",
          "shell.execute_reply.started": "2024-03-21T11:51:15.638757Z",
          "shell.execute_reply": "2024-03-21T11:51:15.653175Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerationLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        embedding_dim,\n",
        "        hidden_size,\n",
        "        num_layers\n",
        "    ):\n",
        "        super(TextGenerationLSTM, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # nn.init.xavier_uniform_(self.lstm.weight)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        if hidden == None:\n",
        "            hidden = self.init_hidden(x.shape[0])\n",
        "        x = self.embedding(x)\n",
        "        out, (h_n, c_n) = self.lstm(x, hidden)\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, (h_n, c_n)\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return h0, c0"
      ],
      "metadata": {
        "id": "gyVHSZrpJYWa",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:15.656444Z",
          "iopub.execute_input": "2024-03-21T11:51:15.656786Z",
          "iopub.status.idle": "2024-03-21T11:51:15.667505Z",
          "shell.execute_reply.started": "2024-03-21T11:51:15.656761Z",
          "shell.execute_reply": "2024-03-21T11:51:15.666462Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установим гиперпараметры"
      ],
      "metadata": {
        "id": "9GFn2rz0Jt3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Setup\n",
        "embedding_dim = 32\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "learning_rate = 0.01\n",
        "epochs = 6"
      ],
      "metadata": {
        "id": "y-_sh96BJe2e",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:15.66913Z",
          "iopub.execute_input": "2024-03-21T11:51:15.669654Z",
          "iopub.status.idle": "2024-03-21T11:51:15.685829Z",
          "shell.execute_reply.started": "2024-03-21T11:51:15.669613Z",
          "shell.execute_reply": "2024-03-21T11:51:15.685055Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextGenerationLSTM(\n",
        "    vocab_size,\n",
        "    embedding_dim,\n",
        "    hidden_size,\n",
        "    num_layers\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "uFO7uIuBJg2F",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:15.686982Z",
          "iopub.execute_input": "2024-03-21T11:51:15.687615Z",
          "iopub.status.idle": "2024-03-21T11:51:15.822437Z",
          "shell.execute_reply.started": "2024-03-21T11:51:15.687573Z",
          "shell.execute_reply": "2024-03-21T11:51:15.821707Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучим модель"
      ],
      "metadata": {
        "id": "7l7xh4WUJpTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(model, epochs, dataloader, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for input_seq, target_seq in dataloader:\n",
        "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
        "            outputs, _ = model(input_seq)\n",
        "            loss = criterion(outputs, target_seq.view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.detach().cpu().numpy()\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")\n",
        "train(model, epochs, dataloader, criterion)"
      ],
      "metadata": {
        "id": "dZqBEnAtJixz",
        "execution": {
          "iopub.status.busy": "2024-03-21T11:51:15.828292Z",
          "iopub.execute_input": "2024-03-21T11:51:15.828577Z",
          "iopub.status.idle": "2024-03-21T13:42:50.772617Z",
          "shell.execute_reply.started": "2024-03-21T11:51:15.828554Z",
          "shell.execute_reply": "2024-03-21T13:42:50.771603Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "EnddYnexJoft",
        "execution": {
          "iopub.status.busy": "2024-03-21T13:42:50.774021Z",
          "iopub.execute_input": "2024-03-21T13:42:50.774642Z",
          "iopub.status.idle": "2024-03-21T13:42:50.779997Z",
          "shell.execute_reply.started": "2024-03-21T13:42:50.774603Z",
          "shell.execute_reply": "2024-03-21T13:42:50.778933Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Делаем предсказания"
      ],
      "metadata": {
        "id": "-BGOynuHJ7k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = '/kaggle/working/word_lstm_5_epoch.net'\n",
        "torch.save(model, model_name)"
      ],
      "metadata": {
        "id": "0li8wL2dKGFv",
        "execution": {
          "iopub.status.busy": "2024-03-21T13:51:20.22435Z",
          "iopub.execute_input": "2024-03-21T13:51:20.224755Z",
          "iopub.status.idle": "2024-03-21T13:51:20.330382Z",
          "shell.execute_reply.started": "2024-03-21T13:51:20.224723Z",
          "shell.execute_reply": "2024-03-21T13:51:20.329258Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T13:51:54.09655Z",
          "iopub.execute_input": "2024-03-21T13:51:54.096917Z",
          "iopub.status.idle": "2024-03-21T13:51:54.102841Z",
          "shell.execute_reply.started": "2024-03-21T13:51:54.096886Z",
          "shell.execute_reply": "2024-03-21T13:51:54.101978Z"
        },
        "trusted": true,
        "id": "Sj6CljIDTF-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_word = torch.load(model_name)"
      ],
      "metadata": {
        "id": "FPsOIDj7KiZj",
        "execution": {
          "iopub.status.busy": "2024-03-21T13:42:50.878653Z",
          "iopub.execute_input": "2024-03-21T13:42:50.878965Z",
          "iopub.status.idle": "2024-03-21T13:42:50.904713Z",
          "shell.execute_reply.started": "2024-03-21T13:42:50.878939Z",
          "shell.execute_reply": "2024-03-21T13:42:50.903939Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, num_words):\n",
        "    model.eval()\n",
        "    words = start_string.split()\n",
        "    for _ in range(num_words):\n",
        "        input_seq = torch.LongTensor([word_to_int[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0).to(device)\n",
        "        h, c = model.init_hidden(1)\n",
        "        output, (h, c) = model(input_seq, (h, c))\n",
        "        next_token = output.argmax(1)[-1].item()\n",
        "        words.append(int_to_word[next_token])\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Example usage:\n",
        "print(\"Generated Text:\", generate_text(loaded_word, start_string=\"Талант сам по себе\", num_words=1000))"
      ],
      "metadata": {
        "id": "53dljl7hJ6-g",
        "execution": {
          "iopub.status.busy": "2024-03-21T13:42:50.905738Z",
          "iopub.execute_input": "2024-03-21T13:42:50.906002Z",
          "iopub.status.idle": "2024-03-21T13:42:52.023973Z",
          "shell.execute_reply.started": "2024-03-21T13:42:50.905978Z",
          "shell.execute_reply": "2024-03-21T13:42:52.023004Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Выводы:**\n",
        "\n",
        "- Построены две модели разных подходов: посимвольной генерации и пословной.\n",
        "- Метрики моделей разнятся. Результат лучше у подхода пословной генерации\n",
        "- Конечный результат далёк от идеального. Возможно это из-за выбранного текста: объёмный, большое количество слов придумано автором, в данном случае, мы считываем перевод английского изначально достаточно сложного текста, который принципиально непереводим. Так что возможно это повлияло на результат,либо возможно следовало усложнить модели."
      ],
      "metadata": {
        "id": "tePY0txNfBe7"
      }
    }
  ]
}